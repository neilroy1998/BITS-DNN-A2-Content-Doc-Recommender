{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:24.965309Z",
     "start_time": "2025-03-04T17:15:24.952910Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install PyPDF2 beautifulsoup4 nltk scikit-learn matplotlib",
   "id": "92d2e1437d7de89c",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:24.979927Z",
     "start_time": "2025-03-04T17:15:24.974306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "4816b6cfa96b7b4c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rajitroy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rajitroy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rajitroy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# ================================"
   ],
   "id": "d2e44442ead75dd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.005716Z",
     "start_time": "2025-03-04T17:15:25.002053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text content from various file types (HTML, PDF, TXT)\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from file: \", file_path)\n",
    "    try:\n",
    "        # Handle different file types\n",
    "        if file_path.endswith('.html'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Parse HTML with BeautifulSoup\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        elif file_path.endswith('.pdf'):\n",
    "            # Use PyPDF2 for PDF files\n",
    "            import PyPDF2\n",
    "\n",
    "            with open(file_path, 'rb') as file:  # Note the 'rb' mode for binary files\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:  # Some pages might not have extractable text\n",
    "                        text += page_text + \" \"\n",
    "\n",
    "        else:  # For text files\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return \"\""
   ],
   "id": "ca67a4ef42ba45ce",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.031201Z",
     "start_time": "2025-03-04T17:15:25.028164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_documents(base_dir):\n",
    "    \"\"\"\n",
    "    Load all documents from the specified directory structure\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Base directory containing subdirectories for categories\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with document IDs as keys and document info as values\n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    doc_id = 0\n",
    "\n",
    "    # Create dictionary to store document paths\n",
    "    doc_paths = {}\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        category = os.path.basename(root)\n",
    "\n",
    "        for file in files:\n",
    "            # Only process HTML, PDF, and text files\n",
    "            if file.endswith(('.html', '.txt', '.csv', '.pdf')):\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Extract text using our universal extractor for all file types\n",
    "                text = extract_text_from_file(file_path)\n",
    "\n",
    "                # Skip if no text was extracted\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                # Store document info\n",
    "                doc_name = f\"{category}/{file}\"\n",
    "                documents[doc_id] = {\n",
    "                    'id': doc_id,\n",
    "                    'name': doc_name,\n",
    "                    'category': category,\n",
    "                    'path': file_path,\n",
    "                    'text': text,\n",
    "                    'tokens': None,  # Will be populated during preprocessing\n",
    "                    'term_freq': None,  # Will be populated during TF-IDF calculation\n",
    "                }\n",
    "                doc_paths[doc_id] = file_path\n",
    "                doc_id += 1\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents from {base_dir}\")\n",
    "    return documents, doc_paths"
   ],
   "id": "48e8fc91280d38e9",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.041011Z",
     "start_time": "2025-03-04T17:15:25.038191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text: tokenize, remove stopwords, punctuation, and stem\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed tokens\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n"
   ],
   "id": "eeb19eb849502364",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.059007Z",
     "start_time": "2025-03-04T17:15:25.056915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_documents(documents):\n",
    "    \"\"\"\n",
    "    Preprocess all documents in the collection\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary with preprocessed tokens\n",
    "    \"\"\"\n",
    "    for doc_id, doc in documents.items():\n",
    "        doc['tokens'] = preprocess_text(doc['text'])\n",
    "\n",
    "    return documents\n"
   ],
   "id": "ea4df493c448174b",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. INVERTED INDEX AND TF-IDF\n",
    "# ============================\n"
   ],
   "id": "f710acd0c7eb85e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.090852Z",
     "start_time": "2025-03-04T17:15:25.088802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_inverted_index(documents):\n",
    "    \"\"\"\n",
    "    Build an inverted index mapping terms to documents\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Inverted index mapping terms to document IDs\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    for doc_id, doc in documents.items():\n",
    "        # Get unique terms in the document\n",
    "        unique_terms = set(doc['tokens'])\n",
    "\n",
    "        # Add document to the posting list of each term\n",
    "        for term in unique_terms:\n",
    "            inverted_index[term].append(doc_id)\n",
    "\n",
    "    return dict(inverted_index)\n"
   ],
   "id": "b04763661fbd3cc6",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.107004Z",
     "start_time": "2025-03-04T17:15:25.103963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_term_frequencies(documents):\n",
    "    \"\"\"\n",
    "    Calculate term frequencies for each document\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary with term frequencies\n",
    "    \"\"\"\n",
    "    for doc_id, doc in documents.items():\n",
    "        # Count term frequencies\n",
    "        term_freq = Counter(doc['tokens'])\n",
    "        doc['term_freq'] = term_freq\n",
    "\n",
    "    return documents\n"
   ],
   "id": "590e4cc1928727eb",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.129037Z",
     "start_time": "2025-03-04T17:15:25.121989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_tfidf(documents, inverted_index):\n",
    "    \"\"\"\n",
    "    Calculate TF-IDF scores for all terms in all documents\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "\n",
    "    Returns:\n",
    "        dict: TF-IDF scores for all terms in all documents\n",
    "        dict: Document vectors for similarity calculations\n",
    "    \"\"\"\n",
    "    N = len(documents)  # Total number of documents\n",
    "\n",
    "    # Calculate IDF for each term\n",
    "    idf = {}\n",
    "    for term, doc_ids in inverted_index.items():\n",
    "        idf[term] = math.log10(N / len(doc_ids))\n",
    "\n",
    "    # Calculate TF-IDF for each term in each document\n",
    "    tfidf = {}\n",
    "    doc_vectors = {}\n",
    "\n",
    "    for doc_id, doc in documents.items():\n",
    "        tfidf[doc_id] = {}\n",
    "        vector = {}\n",
    "\n",
    "        # Get document length (total number of terms)\n",
    "        doc_length = len(doc['tokens'])\n",
    "\n",
    "        # Calculate TF-IDF for each term in the document\n",
    "        for term, freq in doc['term_freq'].items():\n",
    "            # Normalized TF (term frequency / document length)\n",
    "            normalized_tf = freq / doc_length\n",
    "\n",
    "            # TF-IDF score\n",
    "            tfidf[doc_id][term] = normalized_tf * idf.get(term, 0)\n",
    "            vector[term] = tfidf[doc_id][term]\n",
    "\n",
    "        # Store the document vector\n",
    "        doc_vectors[doc_id] = vector\n",
    "\n",
    "    return tfidf, doc_vectors\n"
   ],
   "id": "baafba5dd2874e4b",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.135977Z",
     "start_time": "2025-03-04T17:15:25.132982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_inverted_index(inverted_index, top_n=10):\n",
    "    \"\"\"\n",
    "    Display the inverted index (sorted)\n",
    "\n",
    "    Args:\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "        top_n (int): Number of terms to display\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Sort terms by their frequency (number of documents)\n",
    "    sorted_terms = sorted(inverted_index.items(),\n",
    "                          key=lambda x: len(x[1]),\n",
    "                          reverse=True)\n",
    "\n",
    "    print(f\"Top {top_n} terms in the inverted index:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"{:<20} {:<10} {:<20}\".format(\"Term\", \"Doc Count\", \"Documents\"))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for term, doc_ids in sorted_terms[:top_n]:\n",
    "        print(\"{:<20} {:<10} {:<20}\".format(\n",
    "            term, len(doc_ids), str(doc_ids[:5]) + \"...\" if len(doc_ids) > 5 else str(doc_ids)\n",
    "        ))\n"
   ],
   "id": "74d0527ea3d44aee",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. SIMILARITY CALCULATION\n",
    "# =========================\n"
   ],
   "id": "d07a9058dd5c004"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.146575Z",
     "start_time": "2025-03-04T17:15:25.142864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_similarity_matrix(doc_vectors):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity matrix for document pairs\n",
    "\n",
    "    Args:\n",
    "        doc_vectors (dict): Document vectors\n",
    "\n",
    "    Returns:\n",
    "        dict: Similarity matrix for document pairs\n",
    "    \"\"\"\n",
    "    doc_ids = list(doc_vectors.keys())\n",
    "    similarity_matrix = {}\n",
    "\n",
    "    for i, doc_id1 in enumerate(doc_ids):\n",
    "        similarity_matrix[doc_id1] = {}\n",
    "        vec1 = doc_vectors[doc_id1]\n",
    "\n",
    "        for doc_id2 in doc_ids:\n",
    "            if doc_id1 == doc_id2:\n",
    "                similarity_matrix[doc_id1][doc_id2] = 1.0\n",
    "                continue\n",
    "\n",
    "            vec2 = doc_vectors[doc_id2]\n",
    "\n",
    "            # Calculate dot product\n",
    "            dot_product = 0\n",
    "            for term, tfidf1 in vec1.items():\n",
    "                if term in vec2:\n",
    "                    dot_product += tfidf1 * vec2[term]\n",
    "\n",
    "            # Calculate magnitudes\n",
    "            mag1 = math.sqrt(sum(tfidf**2 for tfidf in vec1.values()))\n",
    "            mag2 = math.sqrt(sum(tfidf**2 for tfidf in vec2.values()))\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            if mag1 * mag2 == 0:\n",
    "                similarity_matrix[doc_id1][doc_id2] = 0\n",
    "            else:\n",
    "                similarity_matrix[doc_id1][doc_id2] = dot_product / (mag1 * mag2)\n",
    "\n",
    "    return similarity_matrix\n"
   ],
   "id": "dc59aa1f6069119f",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.161261Z",
     "start_time": "2025-03-04T17:15:25.157956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_most_similar_documents(similarity_matrix, doc_id, top_n=5):\n",
    "    \"\"\"\n",
    "    Get the most similar documents to a given document\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (dict): Similarity matrix for document pairs\n",
    "        doc_id (int): Document ID\n",
    "        top_n (int): Number of similar documents to return\n",
    "\n",
    "    Returns:\n",
    "        list: Top similar documents with similarity scores\n",
    "    \"\"\"\n",
    "    similarities = similarity_matrix[doc_id]\n",
    "\n",
    "    # Sort by similarity score (descending)\n",
    "    sorted_similarities = sorted(similarities.items(),\n",
    "                                 key=lambda x: x[1],\n",
    "                                 reverse=True)\n",
    "\n",
    "    # Exclude the document itself (similarity = 1.0)\n",
    "    similar_docs = [(doc_id2, score) for doc_id2, score in sorted_similarities\n",
    "                    if doc_id2 != doc_id]\n",
    "\n",
    "    return similar_docs[:top_n]\n"
   ],
   "id": "c44e425def641697",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. RECOMMENDER SYSTEM\n",
    "# =====================\n"
   ],
   "id": "f98cd4d4c833a6da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.175691Z",
     "start_time": "2025-03-04T17:15:25.170631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(query, documents, inverted_index, doc_vectors, user_id=None, user_profiles=None, tolerance=0.8):\n",
    "    \"\"\"\n",
    "    Search for documents matching a query, with optional personalization\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query\n",
    "        documents (dict): Dictionary of documents\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "        doc_vectors (dict): Document vectors for similarity calculations\n",
    "        user_id (str, optional): User ID for personalized results\n",
    "        user_profiles (dict, optional): Dictionary of user profiles\n",
    "        tolerance (float): Tolerance threshold for fuzzy matching\n",
    "\n",
    "    Returns:\n",
    "        list: Ranked list of matching documents\n",
    "    \"\"\"\n",
    "    # Preprocess the query\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    # If no valid tokens after preprocessing, return empty result\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "\n",
    "    # Find matching documents for each query term\n",
    "    matching_docs = set()\n",
    "\n",
    "    for query_term in query_tokens:\n",
    "        # Try exact matching first\n",
    "        if query_term in inverted_index:\n",
    "            matching_docs.update(inverted_index[query_term])\n",
    "        else:\n",
    "            # Try fuzzy matching if exact match not found\n",
    "            all_terms = list(inverted_index.keys())\n",
    "            close_matches = get_close_matches(query_term, all_terms, n=3, cutoff=tolerance)\n",
    "\n",
    "            for match in close_matches:\n",
    "                matching_docs.update(inverted_index[match])\n",
    "\n",
    "    # If no matching documents found, return empty result\n",
    "    if not matching_docs:\n",
    "        return []\n",
    "\n",
    "    # Calculate query vector\n",
    "    query_vector = {}\n",
    "    for term in query_tokens:\n",
    "        # Use TF-IDF weight if the term is in the corpus, otherwise give it a default weight\n",
    "        query_vector[term] = query_vector.get(term, 0) + 1\n",
    "\n",
    "    # Normalize query vector\n",
    "    query_length = len(query_tokens)\n",
    "    for term in query_vector:\n",
    "        query_vector[term] /= query_length\n",
    "\n",
    "    # Calculate similarity to query for each matching document\n",
    "    similarities = []\n",
    "\n",
    "    for doc_id in matching_docs:\n",
    "        doc_vector = doc_vectors[doc_id]\n",
    "\n",
    "        # Calculate dot product\n",
    "        dot_product = 0\n",
    "        for term, weight in query_vector.items():\n",
    "            if term in doc_vector:\n",
    "                dot_product += weight * doc_vector[term]\n",
    "\n",
    "        # Calculate magnitudes\n",
    "        query_mag = math.sqrt(sum(w**2 for w in query_vector.values()))\n",
    "        doc_mag = math.sqrt(sum(w**2 for w in doc_vector.values()))\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        if query_mag * doc_mag == 0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = dot_product / (query_mag * doc_mag)\n",
    "\n",
    "        similarities.append((doc_id, similarity))\n",
    "\n",
    "    # Sort by similarity score (descending)\n",
    "    ranked_results = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Apply personalized ranking if user_id is provided\n",
    "    if user_id and user_profiles and user_id in user_profiles:\n",
    "        ranked_results = personalized_ranking(ranked_results, user_id, documents, user_profiles)\n",
    "\n",
    "    return ranked_results"
   ],
   "id": "96bb58d8fdc93abb",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.186478Z",
     "start_time": "2025-03-04T17:15:25.181937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_search_results(results, documents, user_id=None, user_profiles=None, top_n=5):\n",
    "    \"\"\"\n",
    "    Display search results with personalization indicators if applicable\n",
    "\n",
    "    Args:\n",
    "        results (list): Ranked list of matching documents\n",
    "        documents (dict): Dictionary of documents\n",
    "        user_id (str, optional): User ID for personalized results\n",
    "        user_profiles (dict, optional): Dictionary of user profiles\n",
    "        top_n (int): Number of results to display\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No matching documents found.\")\n",
    "        return\n",
    "\n",
    "    # Display user profile info if personalization was applied\n",
    "    if user_id and user_profiles and user_id in user_profiles:\n",
    "        user_name = user_profiles[user_id]['name']\n",
    "        print(f\"Personalized results for {user_name} (user_id: {user_id})\")\n",
    "\n",
    "    print(f\"Found {len(results)} matching documents.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (doc_id, score) in enumerate(results[:top_n]):\n",
    "        doc = documents[doc_id]\n",
    "        title = doc['name']\n",
    "        category = doc['category']\n",
    "\n",
    "        # Add personalization indicator if appropriate\n",
    "        personalized_indicator = \"\"\n",
    "        if user_id and user_profiles and user_id in user_profiles:\n",
    "            # Check if this document contains terms from user's interest vector\n",
    "            user_interests = user_profiles[user_id]['interest_vector']\n",
    "            matches = sum(1 for term in user_interests if term in doc['tokens'])\n",
    "            if matches > 3:  # Arbitrary threshold for demonstration\n",
    "                personalized_indicator = \" [Matches your interests]\"\n",
    "\n",
    "        print(f\"Rank {i+1}: {title} [Category: {category}]{personalized_indicator}\")\n",
    "        print(f\"Similarity Score: {score:.4f}\")\n",
    "\n",
    "        # Display snippet (first 150 characters of text)\n",
    "        snippet = doc['text'][:150].strip() + \"...\" if len(doc['text']) > 150 else doc['text']\n",
    "        print(f\"Snippet: {snippet}\")\n",
    "        print(\"-\" * 80)"
   ],
   "id": "331cdc77eac17953",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. USER PROFILES\n",
    "# ========================"
   ],
   "id": "7c6f07eeb67912b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.196747Z",
     "start_time": "2025-03-04T17:15:25.192555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_user_profiles():\n",
    "    \"\"\"\n",
    "    Initialize predefined user profiles with search history.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of user profiles with search history\n",
    "    \"\"\"\n",
    "    user_profiles = {\n",
    "        'user01': {\n",
    "            'name': 'Tech User',\n",
    "            'search_history': [\n",
    "                'apple new products',\n",
    "                'artificial intelligence applications',\n",
    "                'latest smartphone reviews',\n",
    "                'technology news today',\n",
    "                'AI machine learning advances',\n",
    "                'tech company stock prices',\n",
    "                'new programming languages',\n",
    "                'software development tools',\n",
    "                'coding best practices',\n",
    "                'tech startup funding'\n",
    "            ],\n",
    "            'interest_vector': {}  # Will be populated during processing\n",
    "        },\n",
    "        'user02': {\n",
    "            'name': 'Arts & Travel Enthusiast',\n",
    "            'search_history': [\n",
    "                'art exhibitions near me',\n",
    "                'famous museums in europe',\n",
    "                'travel destinations 2025',\n",
    "                'cultural heritage sites',\n",
    "                'best places to visit this summer',\n",
    "                'art and fashion trends',\n",
    "                'contemporary artists to watch',\n",
    "                'historical landmarks',\n",
    "                'tourism industry news',\n",
    "                'sustainable travel options'\n",
    "            ],\n",
    "            'interest_vector': {}  # Will be populated during processing\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return user_profiles"
   ],
   "id": "6bee6ba3ae36118b",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.206279Z",
     "start_time": "2025-03-04T17:15:25.203186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_user_interest_vectors(user_profiles, inverted_index, documents):\n",
    "    \"\"\"\n",
    "    Build interest vectors for each user based on their search history\n",
    "\n",
    "    Args:\n",
    "        user_profiles (dict): Dictionary of user profiles\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated user profiles with interest vectors\n",
    "    \"\"\"\n",
    "    for user_id, profile in user_profiles.items():\n",
    "        # Initialize interest vector\n",
    "        interest_vector = Counter()\n",
    "\n",
    "        # Process each search query in history\n",
    "        for query in profile['search_history']:\n",
    "            # Preprocess query\n",
    "            query_tokens = preprocess_text(query)\n",
    "\n",
    "            # Add each token to interest vector with weight\n",
    "            for token in query_tokens:\n",
    "                interest_vector[token] += 1\n",
    "\n",
    "        # Normalize interest vector\n",
    "        if interest_vector:\n",
    "            total_weight = sum(interest_vector.values())\n",
    "            for term in interest_vector:\n",
    "                interest_vector[term] /= total_weight\n",
    "\n",
    "        # Store interest vector in user profile\n",
    "        profile['interest_vector'] = dict(interest_vector)\n",
    "\n",
    "    return user_profiles"
   ],
   "id": "c8bac5b528e4efc8",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.217240Z",
     "start_time": "2025-03-04T17:15:25.213821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_profile_similarity(user_id, doc_id, documents, user_profiles):\n",
    "    \"\"\"\n",
    "    Calculate similarity between a user profile and a document\n",
    "\n",
    "    Args:\n",
    "        user_id (str): User ID\n",
    "        doc_id (int): Document ID\n",
    "        documents (dict): Dictionary of documents\n",
    "        user_profiles (dict): Dictionary of user profiles\n",
    "\n",
    "    Returns:\n",
    "        float: Similarity score between user profile and document\n",
    "    \"\"\"\n",
    "    # If user_id is not provided or invalid, return neutral score\n",
    "    if user_id not in user_profiles:\n",
    "        return 0.5\n",
    "\n",
    "    user_profile = user_profiles[user_id]\n",
    "    document = documents[doc_id]\n",
    "    interest_vector = user_profile['interest_vector']\n",
    "\n",
    "    # If interest vector is empty, return neutral score\n",
    "    if not interest_vector:\n",
    "        return 0.5\n",
    "\n",
    "    # Calculate dot product between interest vector and document tokens\n",
    "    dot_product = 0\n",
    "    for term, weight in interest_vector.items():\n",
    "        if term in document['tokens']:\n",
    "            dot_product += weight * 1  # Simplified: just check if term exists\n",
    "\n",
    "    # Normalize by document length for fairness\n",
    "    doc_length = len(document['tokens'])\n",
    "    if doc_length > 0:\n",
    "        similarity = dot_product / math.sqrt(doc_length)\n",
    "    else:\n",
    "        similarity = 0\n",
    "\n",
    "    return similarity"
   ],
   "id": "1793957a0f9d32b8",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.225049Z",
     "start_time": "2025-03-04T17:15:25.221935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def personalized_ranking(results, user_id, documents, user_profiles):\n",
    "    \"\"\"\n",
    "    Re-rank search results based on user profile\n",
    "\n",
    "    Args:\n",
    "        results (list): Original ranked list of (doc_id, similarity) tuples\n",
    "        user_id (str): User ID\n",
    "        documents (dict): Dictionary of documents\n",
    "        user_profiles (dict): Dictionary of user profiles\n",
    "\n",
    "    Returns:\n",
    "        list: Re-ranked list of (doc_id, combined_score) tuples\n",
    "    \"\"\"\n",
    "    # If no user_id provided or invalid, return original ranking\n",
    "    if not user_id or user_id not in user_profiles:\n",
    "        return results\n",
    "\n",
    "    # Calculate combined scores (query relevance + profile similarity)\n",
    "    combined_scores = []\n",
    "\n",
    "    for doc_id, query_similarity in results:\n",
    "        # Calculate profile similarity\n",
    "        profile_similarity = calculate_profile_similarity(user_id, doc_id, documents, user_profiles)\n",
    "\n",
    "        # Combine scores (70% query relevance, 30% profile similarity)\n",
    "        combined_score = (0.7 * query_similarity) + (0.3 * profile_similarity)\n",
    "\n",
    "        combined_scores.append((doc_id, combined_score, query_similarity, profile_similarity))\n",
    "\n",
    "    # Sort by combined score\n",
    "    re_ranked_results = sorted(combined_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return only doc_id and combined score for compatibility with original format\n",
    "    return [(doc_id, score) for doc_id, score, _, _ in re_ranked_results]"
   ],
   "id": "ccd692dcdf27de1a",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. PERFORMANCE EVALUATION\n",
    "# ========================\n"
   ],
   "id": "305c676608dbc407"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.238237Z",
     "start_time": "2025-03-04T17:15:25.229968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_search(test_queries, documents, inverted_index, doc_vectors, user_id=None, user_profiles=None):\n",
    "    \"\"\"\n",
    "    Evaluate search performance using test queries with detailed logging\n",
    "\n",
    "    Args:\n",
    "        test_queries (dict): Dictionary of test queries with relevance judgments\n",
    "        documents (dict): Dictionary of documents\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "        doc_vectors (dict): Document vectors for similarity calculations\n",
    "        user_id (str, optional): User ID for personalized evaluation\n",
    "        user_profiles (dict, optional): Dictionary of user profiles\n",
    "\n",
    "    Returns:\n",
    "        dict: Performance metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'query_id': [],\n",
    "        'query': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'avg_precision': []\n",
    "    }\n",
    "\n",
    "    print(\"\\nDETAILED EVALUATION LOGS:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Number of test queries: {len(test_queries)}\")\n",
    "\n",
    "    # Add user info if personalization is being evaluated\n",
    "    if user_id and user_profiles and user_id in user_profiles:\n",
    "        user_name = user_profiles[user_id]['name']\n",
    "        print(f\"Evaluation for user: {user_name} (user_id: {user_id})\")\n",
    "\n",
    "    for query_id, query_info in test_queries.items():\n",
    "        query = query_info['query']\n",
    "        relevant_docs = set(query_info['relevant_docs'])\n",
    "\n",
    "        print(f\"\\nQuery {query_id}: '{query}'\")\n",
    "        print(f\"Number of relevant documents defined: {len(relevant_docs)}\")\n",
    "        if len(relevant_docs) == 0:\n",
    "            print(\"WARNING: No relevant documents defined for this query!\")\n",
    "\n",
    "        # Add query information to metrics\n",
    "        metrics['query_id'].append(query_id)\n",
    "        metrics['query'].append(query)\n",
    "\n",
    "        # Get search results (with personalization if user_id provided)\n",
    "        results = search(query, documents, inverted_index, doc_vectors, user_id, user_profiles)\n",
    "        retrieved_docs = set([doc_id for doc_id, _ in results])\n",
    "\n",
    "        print(f\"Number of documents retrieved: {len(retrieved_docs)}\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        if retrieved_docs:\n",
    "            intersection = relevant_docs.intersection(retrieved_docs)\n",
    "            precision = len(intersection) / len(retrieved_docs)\n",
    "            print(f\"Intersection size: {len(intersection)}\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            metrics['precision'].append(precision)\n",
    "        else:\n",
    "            print(\"No documents retrieved!\")\n",
    "            metrics['precision'].append(0)\n",
    "\n",
    "        if relevant_docs:\n",
    "            recall = len(relevant_docs.intersection(retrieved_docs)) / len(relevant_docs)\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            metrics['recall'].append(recall)\n",
    "        else:\n",
    "            print(\"No relevant documents defined!\")\n",
    "            metrics['recall'].append(0)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        if metrics['precision'][-1] + metrics['recall'][-1] > 0:\n",
    "            f1 = 2 * metrics['precision'][-1] * metrics['recall'][-1] / (metrics['precision'][-1] + metrics['recall'][-1])\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "            metrics['f1_score'].append(f1)\n",
    "        else:\n",
    "            print(\"F1 Score: 0.0000 (precision and recall are both 0)\")\n",
    "            metrics['f1_score'].append(0)\n",
    "\n",
    "        # Calculate average precision\n",
    "        avg_precision = 0\n",
    "        correct_count = 0\n",
    "\n",
    "        print(\"\\nPrecision at rank calculation:\")\n",
    "        for i, (doc_id, _) in enumerate(results):\n",
    "            rank = i + 1\n",
    "            if doc_id in relevant_docs:\n",
    "                correct_count += 1\n",
    "                precision_at_k = correct_count / rank\n",
    "                avg_precision += precision_at_k\n",
    "                print(f\"  Rank {rank}: Document {doc_id} is relevant, precision at {rank} = {precision_at_k:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Rank {rank}: Document {doc_id} is not relevant\")\n",
    "\n",
    "        if correct_count > 0 and len(relevant_docs) > 0:\n",
    "            avg_precision /= len(relevant_docs)\n",
    "            print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "        else:\n",
    "            avg_precision = 0\n",
    "            print(\"Average Precision: 0.0000 (no relevant documents retrieved)\")\n",
    "\n",
    "        metrics['avg_precision'].append(avg_precision)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {}\n",
    "    for metric_name in list(metrics.keys()):\n",
    "        if metric_name not in ['query_id', 'query']:  # Skip non-numeric fields\n",
    "            values = metrics[metric_name]\n",
    "            avg_value = sum(values) / len(values) if values else 0\n",
    "            avg_metrics[f'avg_{metric_name}'] = avg_value\n",
    "            print(f\"\\nAverage {metric_name}: {avg_value:.4f}\")\n",
    "\n",
    "    # Add average metrics to the original metrics dictionary\n",
    "    metrics.update(avg_metrics)\n",
    "\n",
    "    return metrics"
   ],
   "id": "2f01cc70948c6ac3",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:25.254600Z",
     "start_time": "2025-03-04T17:15:25.252349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_evaluation_results(metrics):\n",
    "    \"\"\"\n",
    "    Display evaluation results\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): Performance metrics\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Search System Performance Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Average Precision: {metrics['avg_precision']:.4f}\")\n",
    "    print(f\"Average Recall: {metrics['avg_recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {metrics['avg_f1_score']:.4f}\")\n",
    "    print(f\"Mean Average Precision (MAP): {metrics['avg_avg_precision']:.4f}\")\n"
   ],
   "id": "be275262283cd375",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Main Execution\n",
    "# =============\n"
   ],
   "id": "aac6822d025f7a6f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-04T17:15:31.192847Z",
     "start_time": "2025-03-04T17:15:25.265516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Set up base directory\n",
    "    base_dir = 'bbc_articles'  # Update this path to your data directory\n",
    "\n",
    "    # 1. Load and preprocess documents\n",
    "    print(\"Loading and preprocessing documents...\")\n",
    "    documents, doc_paths = load_documents(base_dir)\n",
    "    documents = preprocess_documents(documents)\n",
    "\n",
    "    # 2. Build inverted index and calculate TF-IDF\n",
    "    print(\"\\nBuilding inverted index and calculating TF-IDF scores...\")\n",
    "    documents = calculate_term_frequencies(documents)\n",
    "    inverted_index = build_inverted_index(documents)\n",
    "    tfidf, doc_vectors = calculate_tfidf(documents, inverted_index)\n",
    "\n",
    "    # Display inverted index\n",
    "    display_inverted_index(inverted_index)\n",
    "\n",
    "    # 3. Initialize user profiles and build interest vectors\n",
    "    print(\"\\nInitializing user profiles...\")\n",
    "    user_profiles = {\n",
    "        'user01': {\n",
    "            'name': 'Tech & Programming Enthusiast',\n",
    "            'search_history': [\n",
    "                'apple new products',\n",
    "                'artificial intelligence applications',\n",
    "                'latest smartphone reviews',\n",
    "                'python programming language tutorials',\n",
    "                'AI machine learning advances',\n",
    "                'python libraries for data science',\n",
    "                'new programming languages',\n",
    "                'software development tools',\n",
    "                'python web frameworks comparison',\n",
    "                'tech startup funding'\n",
    "            ],\n",
    "            'interest_vector': {}  # Will be populated during processing\n",
    "        },\n",
    "        'user02': {\n",
    "            'name': 'Nature & Wildlife Enthusiast',\n",
    "            'search_history': [\n",
    "                'art exhibitions near me',\n",
    "                'exotic animals in rainforests',\n",
    "                'wildlife photography tips',\n",
    "                'most dangerous snakes in the world',\n",
    "                'python snake habitat and behavior',\n",
    "                'art and nature intersection',\n",
    "                'animal conservation efforts',\n",
    "                'natural history museums',\n",
    "                'endangered reptile species',\n",
    "                'sustainable wildlife tourism'\n",
    "            ],\n",
    "            'interest_vector': {}  # Will be populated during processing\n",
    "        }\n",
    "    }\n",
    "\n",
    "    user_profiles = build_user_interest_vectors(user_profiles, inverted_index, documents)\n",
    "\n",
    "    print(f\"Created {len(user_profiles)} user profiles\")\n",
    "\n",
    "    # Display sample of user interest vectors for verification\n",
    "    for user_id, profile in user_profiles.items():\n",
    "        print(f\"\\nTop interests for {profile['name']} ({user_id}):\")\n",
    "        interests = sorted(profile['interest_vector'].items(), key=lambda x: x[1], reverse=True)\n",
    "        for term, weight in interests[:10]:  # Show top 10 interests\n",
    "            print(f\"  {term}: {weight:.4f}\")\n",
    "\n",
    "    # 4. Test search functionality with user personalization\n",
    "    print(\"\\nTesting search functionality with personalization...\")\n",
    "\n",
    "    # Test the Python query to demonstrate different interpretations\n",
    "    python_query = \"python\"\n",
    "    print(f\"\\nSearch Query: '{python_query}' (should show different results for each user)\")\n",
    "\n",
    "    # Regular search (no personalization)\n",
    "    print(\"\\nRegular Search Results (no personalization):\")\n",
    "    results = search(python_query, documents, inverted_index, doc_vectors)\n",
    "    display_search_results(results, documents)\n",
    "\n",
    "    # Personalized for user01 (Tech & Programming Enthusiast)\n",
    "    print(f\"\\nPersonalized Results for {user_profiles['user01']['name']} (user01):\")\n",
    "    results_user01 = search(python_query, documents, inverted_index, doc_vectors, 'user01', user_profiles)\n",
    "    display_search_results(results_user01, documents, 'user01', user_profiles)\n",
    "\n",
    "    # Personalized for user02 (Nature & Wildlife Enthusiast)\n",
    "    print(f\"\\nPersonalized Results for {user_profiles['user02']['name']} (user02):\")\n",
    "    results_user02 = search(python_query, documents, inverted_index, doc_vectors, 'user02', user_profiles)\n",
    "    display_search_results(results_user02, documents, 'user02', user_profiles)\n",
    "\n",
    "    # Add other standard test queries\n",
    "    standard_test_queries = [\n",
    "        \"technology and artificial intelligence\",\n",
    "        \"travel destinations in Europe\",\n",
    "        \"art exhibitions and culture\"\n",
    "    ]\n",
    "\n",
    "    # Test each standard query for each user\n",
    "    for query in standard_test_queries:\n",
    "        print(f\"\\nSearch Query: '{query}'\")\n",
    "\n",
    "        # Regular search (no personalization)\n",
    "        print(\"\\nRegular Search Results (no personalization):\")\n",
    "        results = search(query, documents, inverted_index, doc_vectors)\n",
    "        display_search_results(results, documents)\n",
    "\n",
    "        # Personalized for user01 (Tech & Programming Enthusiast)\n",
    "        print(f\"\\nPersonalized Results for {user_profiles['user01']['name']} (user01):\")\n",
    "        results_user01 = search(query, documents, inverted_index, doc_vectors, 'user01', user_profiles)\n",
    "        display_search_results(results_user01, documents, 'user01', user_profiles)\n",
    "\n",
    "        # Personalized for user02 (Nature & Wildlife Enthusiast)\n",
    "        print(f\"\\nPersonalized Results for {user_profiles['user02']['name']} (user02):\")\n",
    "        results_user02 = search(query, documents, inverted_index, doc_vectors, 'user02', user_profiles)\n",
    "        display_search_results(results_user02, documents, 'user02', user_profiles)\n",
    "\n",
    "    # 5. Evaluate search performance\n",
    "    print(\"\\nEvaluating search performance...\")\n",
    "\n",
    "    # Now include a python-specific evaluation to quantify the difference\n",
    "    tech_docs = [doc_id for doc_id, doc in documents.items() if doc['category'] == 'technology']\n",
    "    innovation_docs = [doc_id for doc_id, doc in documents.items() if doc['category'] == 'innovation']\n",
    "\n",
    "    # For user01 (Tech & Programming), python should mean programming language\n",
    "    # For user02 (Nature & Wildlife), python should mean snake\n",
    "    python_tech_docs = [doc_id for doc_id, doc in documents.items()\n",
    "                        if 'python' in doc['text'].lower() and\n",
    "                        ('program' in doc['text'].lower() or 'code' in doc['text'].lower() or 'develop' in doc['text'].lower())]\n",
    "\n",
    "    python_snake_docs = [doc_id for doc_id, doc in documents.items()\n",
    "                         if 'python' in doc['text'].lower() and\n",
    "                         ('snake' in doc['text'].lower() or 'reptile' in doc['text'].lower() or 'animal' in doc['text'].lower())]\n",
    "\n",
    "    # Create evaluation queries\n",
    "    eval_queries = {\n",
    "        0: {\n",
    "            'query': \"artificial intelligence\",\n",
    "            'relevant_docs': tech_docs + innovation_docs\n",
    "        },\n",
    "        1: {\n",
    "            'query': \"museums cultural exhibition\",\n",
    "            'relevant_docs': [doc_id for doc_id, doc in documents.items() if doc['category'] in ['arts', 'travel']]\n",
    "        },\n",
    "        2: {\n",
    "            'query': \"python\",\n",
    "            'relevant_docs': python_tech_docs + python_snake_docs  # All python docs are relevant for generic search\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # User-specific evaluation queries\n",
    "    eval_queries_user01 = {\n",
    "        0: {\n",
    "            'query': \"artificial intelligence\",\n",
    "            'relevant_docs': tech_docs + innovation_docs\n",
    "        },\n",
    "        1: {\n",
    "            'query': \"museums cultural exhibition\",\n",
    "            'relevant_docs': [doc_id for doc_id, doc in documents.items() if doc['category'] in ['arts', 'travel']]\n",
    "        },\n",
    "        2: {\n",
    "            'query': \"python\",\n",
    "            'relevant_docs': python_tech_docs  # Only programming python docs are relevant for tech user\n",
    "        }\n",
    "    }\n",
    "\n",
    "    eval_queries_user02 = {\n",
    "        0: {\n",
    "            'query': \"artificial intelligence\",\n",
    "            'relevant_docs': tech_docs + innovation_docs\n",
    "        },\n",
    "        1: {\n",
    "            'query': \"museums cultural exhibition\",\n",
    "            'relevant_docs': [doc_id for doc_id, doc in documents.items() if doc['category'] in ['arts', 'travel']]\n",
    "        },\n",
    "        2: {\n",
    "            'query': \"python\",\n",
    "            'relevant_docs': python_snake_docs  # Only snake python docs are relevant for nature user\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Evaluate for each user\n",
    "    print(\"\\nPerformance for generic search (no personalization):\")\n",
    "    metrics_generic = evaluate_search(eval_queries, documents, inverted_index, doc_vectors)\n",
    "    display_evaluation_results(metrics_generic)\n",
    "\n",
    "    print(f\"\\nPerformance for {user_profiles['user01']['name']} (user01):\")\n",
    "    metrics_user01 = evaluate_search(eval_queries_user01, documents, inverted_index, doc_vectors, 'user01', user_profiles)\n",
    "    display_evaluation_results(metrics_user01)\n",
    "\n",
    "    print(f\"\\nPerformance for {user_profiles['user02']['name']} (user02):\")\n",
    "    metrics_user02 = evaluate_search(eval_queries_user02, documents, inverted_index, doc_vectors, 'user02', user_profiles)\n",
    "    display_evaluation_results(metrics_user02)\n",
    "\n",
    "    print(\"\\nRecommender System complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing documents...\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_10.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_8.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_9.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_4.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_5.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_7.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_6.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_6.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_7.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_5.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_4.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_9.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_8.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_15.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_5.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_4.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_14.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_6.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_16.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_7.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_13.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_12.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_10.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_11.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_9.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_8.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_9.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_8.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_5.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_4.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_6.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_7.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_13.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_12.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_10.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_11.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_14.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_6.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_5.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_4.pdf\n",
      "Loaded 60 documents from bbc_articles\n",
      "\n",
      "Building inverted index and calculating TF-IDF scores...\n",
      "Top 10 terms in the inverted index:\n",
      "==================================================\n",
      "Term                 Doc Count  Documents           \n",
      "--------------------------------------------------\n",
      "privaci              60         [0, 1, 2, 3, 4]...  \n",
      "pm                   60         [0, 1, 2, 3, 4]...  \n",
      "share                60         [0, 1, 2, 3, 4]...  \n",
      "use                  60         [0, 1, 2, 3, 4]...  \n",
      "ago                  60         [0, 1, 2, 3, 4]...  \n",
      "copyright            60         [0, 1, 2, 3, 4]...  \n",
      "term                 60         [0, 1, 2, 3, 4]...  \n",
      "polici               60         [0, 1, 2, 3, 4]...  \n",
      "approach             60         [0, 1, 2, 3, 4]...  \n",
      "link                 60         [0, 1, 2, 3, 4]...  \n",
      "\n",
      "Initializing user profiles...\n",
      "Created 2 user profiles\n",
      "\n",
      "Top interests for Tech & Programming Enthusiast (user01):\n",
      "  python: 0.0882\n",
      "  new: 0.0588\n",
      "  program: 0.0588\n",
      "  languag: 0.0588\n",
      "  appl: 0.0294\n",
      "  product: 0.0294\n",
      "  artifici: 0.0294\n",
      "  intellig: 0.0294\n",
      "  applic: 0.0294\n",
      "  latest: 0.0294\n",
      "\n",
      "Top interests for Nature & Wildlife Enthusiast (user02):\n",
      "  art: 0.0645\n",
      "  anim: 0.0645\n",
      "  wildlif: 0.0645\n",
      "  snake: 0.0645\n",
      "  natur: 0.0645\n",
      "  exhibit: 0.0323\n",
      "  near: 0.0323\n",
      "  exot: 0.0323\n",
      "  rainforest: 0.0323\n",
      "  photographi: 0.0323\n",
      "\n",
      "Testing search functionality with personalization...\n",
      "\n",
      "Search Query: 'python' (should show different results for each user)\n",
      "\n",
      "Regular Search Results (no personalization):\n",
      "Found 4 matching documents.\n",
      "================================================================================\n",
      "Rank 1: travel/travel_news_6.pdf [Category: travel]\n",
      "Similarity Score: 0.4015\n",
      "Snippet: Royal python found abandoned outside block of flats 11 February 2025Share Save Lewis Adams BBC News, Essex RSPCA Royal pythons originate from West Afr...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: technology/technology_news_15.pdf [Category: technology]\n",
      "Similarity Score: 0.4015\n",
      "Snippet: Royal python found abandoned outside block of flats 11 February 2025Share Save Lewis Adams BBC News, Essex RSPCA Royal pythons originate from West Afr...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: travel/travel_news_5.pdf [Category: travel]\n",
      "Similarity Score: 0.1861\n",
      "Snippet: Park warning after escaped python sightings 13 August 2024Share Save Fosiya Ismail BBC News, West Midlands Getty Images Park visitors were urged not t...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: technology/technology_news_16.pdf [Category: technology]\n",
      "Similarity Score: 0.0456\n",
      "Snippet: Learning to code 'will seriously change your life' 26 March 2021Share Save Michael Winrow Technology of Business reporter Hannah Blair Hannah Blair st...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Personalized Results for Tech & Programming Enthusiast (user01):\n",
      "Personalized results for Tech & Programming Enthusiast (user_id: user01)\n",
      "Found 4 matching documents.\n",
      "================================================================================\n",
      "Rank 1: travel/travel_news_6.pdf [Category: travel]\n",
      "Similarity Score: 0.2835\n",
      "Snippet: Royal python found abandoned outside block of flats 11 February 2025Share Save Lewis Adams BBC News, Essex RSPCA Royal pythons originate from West Afr...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: technology/technology_news_15.pdf [Category: technology]\n",
      "Similarity Score: 0.2835\n",
      "Snippet: Royal python found abandoned outside block of flats 11 February 2025Share Save Lewis Adams BBC News, Essex RSPCA Royal pythons originate from West Afr...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: travel/travel_news_5.pdf [Category: travel]\n",
      "Similarity Score: 0.1324\n",
      "Snippet: Park warning after escaped python sightings 13 August 2024Share Save Fosiya Ismail BBC News, West Midlands Getty Images Park visitors were urged not t...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: technology/technology_news_16.pdf [Category: technology] [Matches your interests]\n",
      "Similarity Score: 0.0367\n",
      "Snippet: Learning to code 'will seriously change your life' 26 March 2021Share Save Michael Winrow Technology of Business reporter Hannah Blair Hannah Blair st...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Personalized Results for Nature & Wildlife Enthusiast (user02):\n",
      "Personalized results for Nature & Wildlife Enthusiast (user_id: user02)\n",
      "Found 4 matching documents.\n",
      "================================================================================\n",
      "Rank 1: travel/travel_news_6.pdf [Category: travel] [Matches your interests]\n",
      "Similarity Score: 0.2853\n",
      "Snippet: Royal python found abandoned outside block of flats 11 February 2025Share Save Lewis Adams BBC News, Essex RSPCA Royal pythons originate from West Afr...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: technology/technology_news_15.pdf [Category: technology] [Matches your interests]\n",
      "Similarity Score: 0.2853\n",
      "Snippet: Royal python found abandoned outside block of flats 11 February 2025Share Save Lewis Adams BBC News, Essex RSPCA Royal pythons originate from West Afr...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: travel/travel_news_5.pdf [Category: travel] [Matches your interests]\n",
      "Similarity Score: 0.1344\n",
      "Snippet: Park warning after escaped python sightings 13 August 2024Share Save Fosiya Ismail BBC News, West Midlands Getty Images Park visitors were urged not t...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: technology/technology_news_16.pdf [Category: technology]\n",
      "Similarity Score: 0.0327\n",
      "Snippet: Learning to code 'will seriously change your life' 26 March 2021Share Save Michael Winrow Technology of Business reporter Hannah Blair Hannah Blair st...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Search Query: 'technology and artificial intelligence'\n",
      "\n",
      "Regular Search Results (no personalization):\n",
      "Found 31 matching documents.\n",
      "================================================================================\n",
      "Rank 1: innovation/innovation_news_9.pdf [Category: innovation]\n",
      "Similarity Score: 0.0666\n",
      "Snippet: Apple commits to $500bn US investment 1 day ago Natalie Sherman Business reporter, BBC News Getty Images Apple plans to invest more than $500bn (£396b...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: innovation/innovation_news_1.pdf [Category: innovation]\n",
      "Similarity Score: 0.0637\n",
      "Snippet: Apple AI tool transcribed the word 'racist' as 'Trump' 2 hours ago Imran Rahman-Jones Technology reporter Reuters Apple says it is working to fix its...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: technology/technology_news_0.pdf [Category: technology]\n",
      "Similarity Score: 0.0637\n",
      "Snippet: Apple AI tool transcribed the word 'racist' as 'Trump' 2 hours ago Imran Rahman-Jones Technology reporter Reuters Apple says it is working to fix its...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: business/business_news_2.pdf [Category: business]\n",
      "Similarity Score: 0.0637\n",
      "Snippet: Apple AI tool transcribed the word 'racist' as 'Trump' 2 hours ago Imran Rahman-Jones Technology reporter Reuters Apple says it is working to fix its...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: innovation/innovation_news_8.pdf [Category: innovation]\n",
      "Similarity Score: 0.0563\n",
      "Snippet: Major Asia bank to cut 4,000 roles as AI replaces humans 1 day ago Peter Hoskins Business reporter Getty Images Singapore's biggest bank, DBS, says it...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Personalized Results for Tech & Programming Enthusiast (user01):\n",
      "Personalized results for Tech & Programming Enthusiast (user_id: user01)\n",
      "Found 31 matching documents.\n",
      "================================================================================\n",
      "Rank 1: innovation/innovation_news_9.pdf [Category: innovation] [Matches your interests]\n",
      "Similarity Score: 0.0517\n",
      "Snippet: Apple commits to $500bn US investment 1 day ago Natalie Sherman Business reporter, BBC News Getty Images Apple plans to invest more than $500bn (£396b...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: innovation/innovation_news_1.pdf [Category: innovation] [Matches your interests]\n",
      "Similarity Score: 0.0508\n",
      "Snippet: Apple AI tool transcribed the word 'racist' as 'Trump' 2 hours ago Imran Rahman-Jones Technology reporter Reuters Apple says it is working to fix its...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: technology/technology_news_0.pdf [Category: technology] [Matches your interests]\n",
      "Similarity Score: 0.0508\n",
      "Snippet: Apple AI tool transcribed the word 'racist' as 'Trump' 2 hours ago Imran Rahman-Jones Technology reporter Reuters Apple says it is working to fix its...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: business/business_news_2.pdf [Category: business] [Matches your interests]\n",
      "Similarity Score: 0.0508\n",
      "Snippet: Apple AI tool transcribed the word 'racist' as 'Trump' 2 hours ago Imran Rahman-Jones Technology reporter Reuters Apple says it is working to fix its...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: innovation/innovation_news_8.pdf [Category: innovation] [Matches your interests]\n",
      "Similarity Score: 0.0424\n",
      "Snippet: Major Asia bank to cut 4,000 roles as AI replaces humans 1 day ago Peter Hoskins Business reporter Getty Images Singapore's biggest bank, DBS, says it...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Personalized Results for Nature & Wildlife Enthusiast (user02):\n",
      "Personalized results for Nature & Wildlife Enthusiast (user_id: user02)\n",
      "Found 31 matching documents.\n",
      "================================================================================\n",
      "Rank 1: innovation/innovation_news_9.pdf [Category: innovation]\n",
      "Similarity Score: 0.0471\n",
      "Snippet: Apple commits to $500bn US investment 1 day ago Natalie Sherman Business reporter, BBC News Getty Images Apple plans to invest more than $500bn (£396b...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: innovation/innovation_news_1.pdf [Category: innovation]\n",
      "Similarity Score: 0.0446\n",
      "Snippet: Apple AI tool transcribed the word 'racist' as 'Trump' 2 hours ago Imran Rahman-Jones Technology reporter Reuters Apple says it is working to fix its...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: technology/technology_news_0.pdf [Category: technology]\n",
      "Similarity Score: 0.0446\n",
      "Snippet: Apple AI tool transcribed the word 'racist' as 'Trump' 2 hours ago Imran Rahman-Jones Technology reporter Reuters Apple says it is working to fix its...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: business/business_news_2.pdf [Category: business]\n",
      "Similarity Score: 0.0446\n",
      "Snippet: Apple AI tool transcribed the word 'racist' as 'Trump' 2 hours ago Imran Rahman-Jones Technology reporter Reuters Apple says it is working to fix its...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: innovation/innovation_news_8.pdf [Category: innovation]\n",
      "Similarity Score: 0.0404\n",
      "Snippet: Major Asia bank to cut 4,000 roles as AI replaces humans 1 day ago Peter Hoskins Business reporter Getty Images Singapore's biggest bank, DBS, says it...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Search Query: 'travel destinations in Europe'\n",
      "\n",
      "Regular Search Results (no personalization):\n",
      "Found 28 matching documents.\n",
      "================================================================================\n",
      "Rank 1: travel/travel_news_3.pdf [Category: travel]\n",
      "Similarity Score: 0.1824\n",
      "Snippet: Digital travel permit to be introduced in late 2025 1 day ago Ashlea Tracey BBC News, Isle of Man Manx Scenes The schem e m irrors the system introduc...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: travel/travel_news_4.pdf [Category: travel]\n",
      "Similarity Score: 0.0408\n",
      "Snippet: The over-50s flying high after airline appeal 1 day ago Jody Sabral BBC News, South East Jody Sabral/BBC Race Welch, a retired grandfather, has joined...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: business/business_news_10.pdf [Category: business]\n",
      "Similarity Score: 0.0210\n",
      "Snippet: Trump orders copper probe in first step to tariffs 15 hours ago Natalie Sherman BBC News Getty Images US President Donald Trump has ordered an investi...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: travel/travel_news_0.pdf [Category: travel]\n",
      "Similarity Score: 0.0195\n",
      "Snippet: Farm landing strip has planning exemption refused 1 hour ago Trevor Bevins Local Democracy Reporting Service Court Farm Dorset Council refused to gran...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: travel/travel_news_1.pdf [Category: travel]\n",
      "Similarity Score: 0.0174\n",
      "Snippet: Couple's trauma after body placed next to them on flight 3 hours ago Maia Davies BBC News Watch: Mitchell Ring and Jennifer Colin speak to A Current A...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Personalized Results for Tech & Programming Enthusiast (user01):\n",
      "Personalized results for Tech & Programming Enthusiast (user_id: user01)\n",
      "Found 28 matching documents.\n",
      "================================================================================\n",
      "Rank 1: travel/travel_news_3.pdf [Category: travel]\n",
      "Similarity Score: 0.1285\n",
      "Snippet: Digital travel permit to be introduced in late 2025 1 day ago Ashlea Tracey BBC News, Isle of Man Manx Scenes The schem e m irrors the system introduc...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: travel/travel_news_4.pdf [Category: travel]\n",
      "Similarity Score: 0.0295\n",
      "Snippet: The over-50s flying high after airline appeal 1 day ago Jody Sabral BBC News, South East Jody Sabral/BBC Race Welch, a retired grandfather, has joined...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: business/business_news_10.pdf [Category: business] [Matches your interests]\n",
      "Similarity Score: 0.0169\n",
      "Snippet: Trump orders copper probe in first step to tariffs 15 hours ago Natalie Sherman BBC News Getty Images US President Donald Trump has ordered an investi...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: innovation/innovation_news_9.pdf [Category: innovation] [Matches your interests]\n",
      "Similarity Score: 0.0158\n",
      "Snippet: Apple commits to $500bn US investment 1 day ago Natalie Sherman Business reporter, BBC News Getty Images Apple plans to invest more than $500bn (£396b...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: travel/travel_news_0.pdf [Category: travel]\n",
      "Similarity Score: 0.0142\n",
      "Snippet: Farm landing strip has planning exemption refused 1 hour ago Trevor Bevins Local Democracy Reporting Service Court Farm Dorset Council refused to gran...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Personalized Results for Nature & Wildlife Enthusiast (user02):\n",
      "Personalized results for Nature & Wildlife Enthusiast (user_id: user02)\n",
      "Found 28 matching documents.\n",
      "================================================================================\n",
      "Rank 1: travel/travel_news_3.pdf [Category: travel]\n",
      "Similarity Score: 0.1277\n",
      "Snippet: Digital travel permit to be introduced in late 2025 1 day ago Ashlea Tracey BBC News, Isle of Man Manx Scenes The schem e m irrors the system introduc...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: travel/travel_news_4.pdf [Category: travel]\n",
      "Similarity Score: 0.0291\n",
      "Snippet: The over-50s flying high after airline appeal 1 day ago Jody Sabral BBC News, South East Jody Sabral/BBC Race Welch, a retired grandfather, has joined...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: business/business_news_10.pdf [Category: business]\n",
      "Similarity Score: 0.0147\n",
      "Snippet: Trump orders copper probe in first step to tariffs 15 hours ago Natalie Sherman BBC News Getty Images US President Donald Trump has ordered an investi...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: travel/travel_news_0.pdf [Category: travel]\n",
      "Similarity Score: 0.0142\n",
      "Snippet: Farm landing strip has planning exemption refused 1 hour ago Trevor Bevins Local Democracy Reporting Service Court Farm Dorset Council refused to gran...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: arts/arts_news_2.pdf [Category: arts]\n",
      "Similarity Score: 0.0135\n",
      "Snippet: Sites sought for Ukranian bombed door sculpture 3 hours ago Robert Marshall The mangled front door from a destroyed house in eastern Ukraine has a gap...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Search Query: 'art exhibitions and culture'\n",
      "\n",
      "Regular Search Results (no personalization):\n",
      "Found 21 matching documents.\n",
      "================================================================================\n",
      "Rank 1: arts/arts_news_3.pdf [Category: arts]\n",
      "Similarity Score: 0.4026\n",
      "Snippet: Abuse survivors' art is 'relief from a monster' 6 hours ago Sophie Parker BBC News, Wiltshire Wiltshire Council The exhibition has work on display fro...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: arts/arts_news_8.pdf [Category: arts]\n",
      "Similarity Score: 0.2001\n",
      "Snippet: Westminster visit to highlight city's culture 1 day ago PA Media Hull was the UK City of Culture in 2017 Delegates from Hull are appearing at an event...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: arts/arts_news_0.pdf [Category: arts]\n",
      "Similarity Score: 0.1938\n",
      "Snippet: £50k reward for 'great' artist Orlik's missing works 18 February 2025 Sophie Parker BBC News, Wiltshire Henry OrlikHomeNewsSportBusinessInnovationCult...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: arts/arts_news_4.pdf [Category: arts]\n",
      "Similarity Score: 0.0390\n",
      "Snippet: Keith Flint mural to be restored to former glory 21 hours ago George King BBC News, Essex Jaime Bunting Photography The Keith Flint mural at Braintree...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: arts/arts_news_2.pdf [Category: arts]\n",
      "Similarity Score: 0.0367\n",
      "Snippet: Sites sought for Ukranian bombed door sculpture 3 hours ago Robert Marshall The mangled front door from a destroyed house in eastern Ukraine has a gap...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Personalized Results for Tech & Programming Enthusiast (user01):\n",
      "Personalized results for Tech & Programming Enthusiast (user_id: user01)\n",
      "Found 21 matching documents.\n",
      "================================================================================\n",
      "Rank 1: arts/arts_news_3.pdf [Category: arts]\n",
      "Similarity Score: 0.2826\n",
      "Snippet: Abuse survivors' art is 'relief from a monster' 6 hours ago Sophie Parker BBC News, Wiltshire Wiltshire Council The exhibition has work on display fro...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: arts/arts_news_8.pdf [Category: arts]\n",
      "Similarity Score: 0.1419\n",
      "Snippet: Westminster visit to highlight city's culture 1 day ago PA Media Hull was the UK City of Culture in 2017 Delegates from Hull are appearing at an event...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: arts/arts_news_0.pdf [Category: arts]\n",
      "Similarity Score: 0.1363\n",
      "Snippet: £50k reward for 'great' artist Orlik's missing works 18 February 2025 Sophie Parker BBC News, Wiltshire Henry OrlikHomeNewsSportBusinessInnovationCult...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: arts/arts_news_4.pdf [Category: arts]\n",
      "Similarity Score: 0.0277\n",
      "Snippet: Keith Flint mural to be restored to former glory 21 hours ago George King BBC News, Essex Jaime Bunting Photography The Keith Flint mural at Braintree...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: arts/arts_news_2.pdf [Category: arts]\n",
      "Similarity Score: 0.0257\n",
      "Snippet: Sites sought for Ukranian bombed door sculpture 3 hours ago Robert Marshall The mangled front door from a destroyed house in eastern Ukraine has a gap...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Personalized Results for Nature & Wildlife Enthusiast (user02):\n",
      "Personalized results for Nature & Wildlife Enthusiast (user_id: user02)\n",
      "Found 21 matching documents.\n",
      "================================================================================\n",
      "Rank 1: arts/arts_news_3.pdf [Category: arts]\n",
      "Similarity Score: 0.2831\n",
      "Snippet: Abuse survivors' art is 'relief from a monster' 6 hours ago Sophie Parker BBC News, Wiltshire Wiltshire Council The exhibition has work on display fro...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: arts/arts_news_8.pdf [Category: arts]\n",
      "Similarity Score: 0.1421\n",
      "Snippet: Westminster visit to highlight city's culture 1 day ago PA Media Hull was the UK City of Culture in 2017 Delegates from Hull are appearing at an event...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: arts/arts_news_0.pdf [Category: arts]\n",
      "Similarity Score: 0.1372\n",
      "Snippet: £50k reward for 'great' artist Orlik's missing works 18 February 2025 Sophie Parker BBC News, Wiltshire Henry OrlikHomeNewsSportBusinessInnovationCult...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: arts/arts_news_4.pdf [Category: arts]\n",
      "Similarity Score: 0.0286\n",
      "Snippet: Keith Flint mural to be restored to former glory 21 hours ago George King BBC News, Essex Jaime Bunting Photography The Keith Flint mural at Braintree...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: arts/arts_news_2.pdf [Category: arts]\n",
      "Similarity Score: 0.0271\n",
      "Snippet: Sites sought for Ukranian bombed door sculpture 3 hours ago Robert Marshall The mangled front door from a destroyed house in eastern Ukraine has a gap...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating search performance...\n",
      "\n",
      "Performance for generic search (no personalization):\n",
      "\n",
      "DETAILED EVALUATION LOGS:\n",
      "======================================================================\n",
      "Number of test queries: 3\n",
      "\n",
      "Query 0: 'artificial intelligence'\n",
      "Number of relevant documents defined: 28\n",
      "Number of documents retrieved: 13\n",
      "Intersection size: 9\n",
      "Precision: 0.6923\n",
      "Recall: 0.3214\n",
      "F1 Score: 0.4390\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 2 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 33 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 9 is relevant, precision at 3 = 1.0000\n",
      "  Rank 4: Document 41 is not relevant\n",
      "  Rank 5: Document 4 is relevant, precision at 5 = 0.8000\n",
      "  Rank 6: Document 22 is relevant, precision at 6 = 0.8333\n",
      "  Rank 7: Document 1 is relevant, precision at 7 = 0.8571\n",
      "  Rank 8: Document 49 is not relevant\n",
      "  Rank 9: Document 34 is relevant, precision at 9 = 0.7778\n",
      "  Rank 10: Document 44 is not relevant\n",
      "  Rank 11: Document 43 is not relevant\n",
      "  Rank 12: Document 0 is relevant, precision at 12 = 0.6667\n",
      "  Rank 13: Document 27 is relevant, precision at 13 = 0.6923\n",
      "Average Precision: 0.2724\n",
      "\n",
      "Query 1: 'museums cultural exhibition'\n",
      "Number of relevant documents defined: 17\n",
      "Number of documents retrieved: 18\n",
      "Intersection size: 6\n",
      "Precision: 0.3333\n",
      "Recall: 0.3529\n",
      "F1 Score: 0.3429\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 13 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 20 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 11 is relevant, precision at 3 = 1.0000\n",
      "  Rank 4: Document 19 is relevant, precision at 4 = 1.0000\n",
      "  Rank 5: Document 0 is not relevant\n",
      "  Rank 6: Document 38 is not relevant\n",
      "  Rank 7: Document 10 is not relevant\n",
      "  Rank 8: Document 25 is not relevant\n",
      "  Rank 9: Document 18 is relevant, precision at 9 = 0.5556\n",
      "  Rank 10: Document 4 is not relevant\n",
      "  Rank 11: Document 22 is not relevant\n",
      "  Rank 12: Document 1 is not relevant\n",
      "  Rank 13: Document 49 is not relevant\n",
      "  Rank 14: Document 30 is not relevant\n",
      "  Rank 15: Document 17 is relevant, precision at 15 = 0.4000\n",
      "  Rank 16: Document 5 is not relevant\n",
      "  Rank 17: Document 36 is not relevant\n",
      "  Rank 18: Document 27 is not relevant\n",
      "Average Precision: 0.2915\n",
      "\n",
      "Query 2: 'python'\n",
      "Number of relevant documents defined: 4\n",
      "Number of documents retrieved: 4\n",
      "Intersection size: 4\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 57 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 21 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 58 is relevant, precision at 3 = 1.0000\n",
      "  Rank 4: Document 26 is relevant, precision at 4 = 1.0000\n",
      "Average Precision: 1.0000\n",
      "\n",
      "Average precision: 0.6752\n",
      "\n",
      "Average recall: 0.5581\n",
      "\n",
      "Average f1_score: 0.5940\n",
      "\n",
      "Average avg_precision: 0.5213\n",
      "Search System Performance Evaluation\n",
      "==================================================\n",
      "Average Precision: 0.6752\n",
      "Average Recall: 0.5581\n",
      "Average F1 Score: 0.5940\n",
      "Mean Average Precision (MAP): 0.5213\n",
      "\n",
      "Performance for Tech & Programming Enthusiast (user01):\n",
      "\n",
      "DETAILED EVALUATION LOGS:\n",
      "======================================================================\n",
      "Number of test queries: 3\n",
      "Evaluation for user: Tech & Programming Enthusiast (user_id: user01)\n",
      "\n",
      "Query 0: 'artificial intelligence'\n",
      "Number of relevant documents defined: 28\n",
      "Number of documents retrieved: 13\n",
      "Intersection size: 9\n",
      "Precision: 0.6923\n",
      "Recall: 0.3214\n",
      "F1 Score: 0.4390\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 2 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 33 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 9 is relevant, precision at 3 = 1.0000\n",
      "  Rank 4: Document 41 is not relevant\n",
      "  Rank 5: Document 4 is relevant, precision at 5 = 0.8000\n",
      "  Rank 6: Document 22 is relevant, precision at 6 = 0.8333\n",
      "  Rank 7: Document 1 is relevant, precision at 7 = 0.8571\n",
      "  Rank 8: Document 49 is not relevant\n",
      "  Rank 9: Document 34 is relevant, precision at 9 = 0.7778\n",
      "  Rank 10: Document 44 is not relevant\n",
      "  Rank 11: Document 43 is not relevant\n",
      "  Rank 12: Document 0 is relevant, precision at 12 = 0.6667\n",
      "  Rank 13: Document 27 is relevant, precision at 13 = 0.6923\n",
      "Average Precision: 0.2724\n",
      "\n",
      "Query 1: 'museums cultural exhibition'\n",
      "Number of relevant documents defined: 17\n",
      "Number of documents retrieved: 18\n",
      "Intersection size: 6\n",
      "Precision: 0.3333\n",
      "Recall: 0.3529\n",
      "F1 Score: 0.3429\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 13 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 20 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 11 is relevant, precision at 3 = 1.0000\n",
      "  Rank 4: Document 19 is relevant, precision at 4 = 1.0000\n",
      "  Rank 5: Document 0 is not relevant\n",
      "  Rank 6: Document 38 is not relevant\n",
      "  Rank 7: Document 10 is not relevant\n",
      "  Rank 8: Document 25 is not relevant\n",
      "  Rank 9: Document 18 is relevant, precision at 9 = 0.5556\n",
      "  Rank 10: Document 4 is not relevant\n",
      "  Rank 11: Document 22 is not relevant\n",
      "  Rank 12: Document 1 is not relevant\n",
      "  Rank 13: Document 49 is not relevant\n",
      "  Rank 14: Document 30 is not relevant\n",
      "  Rank 15: Document 17 is relevant, precision at 15 = 0.4000\n",
      "  Rank 16: Document 5 is not relevant\n",
      "  Rank 17: Document 36 is not relevant\n",
      "  Rank 18: Document 27 is not relevant\n",
      "Average Precision: 0.2915\n",
      "\n",
      "Query 2: 'python'\n",
      "Number of relevant documents defined: 1\n",
      "Number of documents retrieved: 4\n",
      "Intersection size: 1\n",
      "Precision: 0.2500\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.4000\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 57 is not relevant\n",
      "  Rank 2: Document 21 is not relevant\n",
      "  Rank 3: Document 58 is not relevant\n",
      "  Rank 4: Document 26 is relevant, precision at 4 = 0.2500\n",
      "Average Precision: 0.2500\n",
      "\n",
      "Average precision: 0.4252\n",
      "\n",
      "Average recall: 0.5581\n",
      "\n",
      "Average f1_score: 0.3940\n",
      "\n",
      "Average avg_precision: 0.2713\n",
      "Search System Performance Evaluation\n",
      "==================================================\n",
      "Average Precision: 0.4252\n",
      "Average Recall: 0.5581\n",
      "Average F1 Score: 0.3940\n",
      "Mean Average Precision (MAP): 0.2713\n",
      "\n",
      "Performance for Nature & Wildlife Enthusiast (user02):\n",
      "\n",
      "DETAILED EVALUATION LOGS:\n",
      "======================================================================\n",
      "Number of test queries: 3\n",
      "Evaluation for user: Nature & Wildlife Enthusiast (user_id: user02)\n",
      "\n",
      "Query 0: 'artificial intelligence'\n",
      "Number of relevant documents defined: 28\n",
      "Number of documents retrieved: 13\n",
      "Intersection size: 9\n",
      "Precision: 0.6923\n",
      "Recall: 0.3214\n",
      "F1 Score: 0.4390\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 2 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 33 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 9 is relevant, precision at 3 = 1.0000\n",
      "  Rank 4: Document 41 is not relevant\n",
      "  Rank 5: Document 4 is relevant, precision at 5 = 0.8000\n",
      "  Rank 6: Document 22 is relevant, precision at 6 = 0.8333\n",
      "  Rank 7: Document 1 is relevant, precision at 7 = 0.8571\n",
      "  Rank 8: Document 49 is not relevant\n",
      "  Rank 9: Document 34 is relevant, precision at 9 = 0.7778\n",
      "  Rank 10: Document 44 is not relevant\n",
      "  Rank 11: Document 43 is not relevant\n",
      "  Rank 12: Document 0 is relevant, precision at 12 = 0.6667\n",
      "  Rank 13: Document 27 is relevant, precision at 13 = 0.6923\n",
      "Average Precision: 0.2724\n",
      "\n",
      "Query 1: 'museums cultural exhibition'\n",
      "Number of relevant documents defined: 17\n",
      "Number of documents retrieved: 18\n",
      "Intersection size: 6\n",
      "Precision: 0.3333\n",
      "Recall: 0.3529\n",
      "F1 Score: 0.3429\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 13 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 20 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 11 is relevant, precision at 3 = 1.0000\n",
      "  Rank 4: Document 19 is relevant, precision at 4 = 1.0000\n",
      "  Rank 5: Document 0 is not relevant\n",
      "  Rank 6: Document 38 is not relevant\n",
      "  Rank 7: Document 10 is not relevant\n",
      "  Rank 8: Document 25 is not relevant\n",
      "  Rank 9: Document 18 is relevant, precision at 9 = 0.5556\n",
      "  Rank 10: Document 4 is not relevant\n",
      "  Rank 11: Document 22 is not relevant\n",
      "  Rank 12: Document 1 is not relevant\n",
      "  Rank 13: Document 49 is not relevant\n",
      "  Rank 14: Document 30 is not relevant\n",
      "  Rank 15: Document 17 is relevant, precision at 15 = 0.4000\n",
      "  Rank 16: Document 27 is not relevant\n",
      "  Rank 17: Document 5 is not relevant\n",
      "  Rank 18: Document 36 is not relevant\n",
      "Average Precision: 0.2915\n",
      "\n",
      "Query 2: 'python'\n",
      "Number of relevant documents defined: 3\n",
      "Number of documents retrieved: 4\n",
      "Intersection size: 3\n",
      "Precision: 0.7500\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.8571\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 57 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 21 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 58 is relevant, precision at 3 = 1.0000\n",
      "  Rank 4: Document 26 is not relevant\n",
      "Average Precision: 1.0000\n",
      "\n",
      "Average precision: 0.5919\n",
      "\n",
      "Average recall: 0.5581\n",
      "\n",
      "Average f1_score: 0.5463\n",
      "\n",
      "Average avg_precision: 0.5213\n",
      "Search System Performance Evaluation\n",
      "==================================================\n",
      "Average Precision: 0.5919\n",
      "Average Recall: 0.5581\n",
      "Average F1 Score: 0.5463\n",
      "Mean Average Precision (MAP): 0.5213\n",
      "\n",
      "Recommender System complete.\n"
     ]
    }
   ],
   "execution_count": 84
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
