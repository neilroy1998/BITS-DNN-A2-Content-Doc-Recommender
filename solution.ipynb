{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.062118Z",
     "start_time": "2025-02-26T16:07:14.059418Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install PyPDF2 beautifulsoup4 nltk scikit-learn matplotlib",
   "id": "92d2e1437d7de89c",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.082812Z",
     "start_time": "2025-02-26T16:07:14.076397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "4816b6cfa96b7b4c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rajitroy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rajitroy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rajitroy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# ================================"
   ],
   "id": "d2e44442ead75dd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.112385Z",
     "start_time": "2025-02-26T16:07:14.108228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text content from various file types (HTML, PDF, TXT)\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from file\")\n",
    "    try:\n",
    "        # Handle different file types\n",
    "        if file_path.endswith('.html'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Parse HTML with BeautifulSoup\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        elif file_path.endswith('.pdf'):\n",
    "            # Use PyPDF2 for PDF files\n",
    "            import PyPDF2\n",
    "\n",
    "            with open(file_path, 'rb') as file:  # Note the 'rb' mode for binary files\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:  # Some pages might not have extractable text\n",
    "                        text += page_text + \" \"\n",
    "\n",
    "        else:  # For text files\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return \"\""
   ],
   "id": "ca67a4ef42ba45ce",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.143742Z",
     "start_time": "2025-02-26T16:07:14.140166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_documents(base_dir):\n",
    "    \"\"\"\n",
    "    Load all documents from the specified directory structure\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Base directory containing subdirectories for categories\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with document IDs as keys and document info as values\n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    doc_id = 0\n",
    "\n",
    "    # Create dictionary to store document paths\n",
    "    doc_paths = {}\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        category = os.path.basename(root)\n",
    "\n",
    "        for file in files:\n",
    "            # Only process HTML, PDF, and text files\n",
    "            if file.endswith(('.html', '.txt', '.csv', '.pdf')):\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Extract text using our universal extractor for all file types\n",
    "                text = extract_text_from_file(file_path)\n",
    "\n",
    "                # Skip if no text was extracted\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                # Store document info\n",
    "                doc_name = f\"{category}_{file}\"\n",
    "                documents[doc_id] = {\n",
    "                    'id': doc_id,\n",
    "                    'name': doc_name,\n",
    "                    'category': category,\n",
    "                    'path': file_path,\n",
    "                    'text': text,\n",
    "                    'tokens': None,  # Will be populated during preprocessing\n",
    "                    'term_freq': None,  # Will be populated during TF-IDF calculation\n",
    "                }\n",
    "                doc_paths[doc_id] = file_path\n",
    "                doc_id += 1\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents from {base_dir}\")\n",
    "    return documents, doc_paths"
   ],
   "id": "48e8fc91280d38e9",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.159700Z",
     "start_time": "2025-02-26T16:07:14.156585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text: tokenize, remove stopwords, punctuation, and stem\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed tokens\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n"
   ],
   "id": "eeb19eb849502364",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.165032Z",
     "start_time": "2025-02-26T16:07:14.162690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_documents(documents):\n",
    "    \"\"\"\n",
    "    Preprocess all documents in the collection\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary with preprocessed tokens\n",
    "    \"\"\"\n",
    "    for doc_id, doc in documents.items():\n",
    "        doc['tokens'] = preprocess_text(doc['text'])\n",
    "\n",
    "    return documents\n"
   ],
   "id": "ea4df493c448174b",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. INVERTED INDEX AND TF-IDF\n",
    "# ============================\n"
   ],
   "id": "f710acd0c7eb85e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.173725Z",
     "start_time": "2025-02-26T16:07:14.170043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_inverted_index(documents):\n",
    "    \"\"\"\n",
    "    Build an inverted index mapping terms to documents\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Inverted index mapping terms to document IDs\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    for doc_id, doc in documents.items():\n",
    "        # Get unique terms in the document\n",
    "        unique_terms = set(doc['tokens'])\n",
    "\n",
    "        # Add document to the posting list of each term\n",
    "        for term in unique_terms:\n",
    "            inverted_index[term].append(doc_id)\n",
    "\n",
    "    return dict(inverted_index)\n"
   ],
   "id": "b04763661fbd3cc6",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.193655Z",
     "start_time": "2025-02-26T16:07:14.187276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_term_frequencies(documents):\n",
    "    \"\"\"\n",
    "    Calculate term frequencies for each document\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary with term frequencies\n",
    "    \"\"\"\n",
    "    for doc_id, doc in documents.items():\n",
    "        # Count term frequencies\n",
    "        term_freq = Counter(doc['tokens'])\n",
    "        doc['term_freq'] = term_freq\n",
    "\n",
    "    return documents\n"
   ],
   "id": "590e4cc1928727eb",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.205138Z",
     "start_time": "2025-02-26T16:07:14.200107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_tfidf(documents, inverted_index):\n",
    "    \"\"\"\n",
    "    Calculate TF-IDF scores for all terms in all documents\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "\n",
    "    Returns:\n",
    "        dict: TF-IDF scores for all terms in all documents\n",
    "        dict: Document vectors for similarity calculations\n",
    "    \"\"\"\n",
    "    N = len(documents)  # Total number of documents\n",
    "\n",
    "    # Calculate IDF for each term\n",
    "    idf = {}\n",
    "    for term, doc_ids in inverted_index.items():\n",
    "        idf[term] = math.log10(N / len(doc_ids))\n",
    "\n",
    "    # Calculate TF-IDF for each term in each document\n",
    "    tfidf = {}\n",
    "    doc_vectors = {}\n",
    "\n",
    "    for doc_id, doc in documents.items():\n",
    "        tfidf[doc_id] = {}\n",
    "        vector = {}\n",
    "\n",
    "        # Get document length (total number of terms)\n",
    "        doc_length = len(doc['tokens'])\n",
    "\n",
    "        # Calculate TF-IDF for each term in the document\n",
    "        for term, freq in doc['term_freq'].items():\n",
    "            # Normalized TF (term frequency / document length)\n",
    "            normalized_tf = freq / doc_length\n",
    "\n",
    "            # TF-IDF score\n",
    "            tfidf[doc_id][term] = normalized_tf * idf.get(term, 0)\n",
    "            vector[term] = tfidf[doc_id][term]\n",
    "\n",
    "        # Store the document vector\n",
    "        doc_vectors[doc_id] = vector\n",
    "\n",
    "    return tfidf, doc_vectors\n"
   ],
   "id": "baafba5dd2874e4b",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.218993Z",
     "start_time": "2025-02-26T16:07:14.214996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_inverted_index(inverted_index, top_n=10):\n",
    "    \"\"\"\n",
    "    Display the inverted index (sorted)\n",
    "\n",
    "    Args:\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "        top_n (int): Number of terms to display\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Sort terms by their frequency (number of documents)\n",
    "    sorted_terms = sorted(inverted_index.items(),\n",
    "                          key=lambda x: len(x[1]),\n",
    "                          reverse=True)\n",
    "\n",
    "    print(f\"Top {top_n} terms in the inverted index:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"{:<20} {:<10} {:<20}\".format(\"Term\", \"Doc Count\", \"Documents\"))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for term, doc_ids in sorted_terms[:top_n]:\n",
    "        print(\"{:<20} {:<10} {:<20}\".format(\n",
    "            term, len(doc_ids), str(doc_ids[:5]) + \"...\" if len(doc_ids) > 5 else str(doc_ids)\n",
    "        ))\n"
   ],
   "id": "74d0527ea3d44aee",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. SIMILARITY CALCULATION\n",
    "# =========================\n"
   ],
   "id": "d07a9058dd5c004"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.225562Z",
     "start_time": "2025-02-26T16:07:14.222684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_similarity_matrix(doc_vectors):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity matrix for document pairs\n",
    "\n",
    "    Args:\n",
    "        doc_vectors (dict): Document vectors\n",
    "\n",
    "    Returns:\n",
    "        dict: Similarity matrix for document pairs\n",
    "    \"\"\"\n",
    "    doc_ids = list(doc_vectors.keys())\n",
    "    similarity_matrix = {}\n",
    "\n",
    "    for i, doc_id1 in enumerate(doc_ids):\n",
    "        similarity_matrix[doc_id1] = {}\n",
    "        vec1 = doc_vectors[doc_id1]\n",
    "\n",
    "        for doc_id2 in doc_ids:\n",
    "            if doc_id1 == doc_id2:\n",
    "                similarity_matrix[doc_id1][doc_id2] = 1.0\n",
    "                continue\n",
    "\n",
    "            vec2 = doc_vectors[doc_id2]\n",
    "\n",
    "            # Calculate dot product\n",
    "            dot_product = 0\n",
    "            for term, tfidf1 in vec1.items():\n",
    "                if term in vec2:\n",
    "                    dot_product += tfidf1 * vec2[term]\n",
    "\n",
    "            # Calculate magnitudes\n",
    "            mag1 = math.sqrt(sum(tfidf**2 for tfidf in vec1.values()))\n",
    "            mag2 = math.sqrt(sum(tfidf**2 for tfidf in vec2.values()))\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            if mag1 * mag2 == 0:\n",
    "                similarity_matrix[doc_id1][doc_id2] = 0\n",
    "            else:\n",
    "                similarity_matrix[doc_id1][doc_id2] = dot_product / (mag1 * mag2)\n",
    "\n",
    "    return similarity_matrix\n"
   ],
   "id": "dc59aa1f6069119f",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.234683Z",
     "start_time": "2025-02-26T16:07:14.231523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_most_similar_documents(similarity_matrix, doc_id, top_n=5):\n",
    "    \"\"\"\n",
    "    Get the most similar documents to a given document\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (dict): Similarity matrix for document pairs\n",
    "        doc_id (int): Document ID\n",
    "        top_n (int): Number of similar documents to return\n",
    "\n",
    "    Returns:\n",
    "        list: Top similar documents with similarity scores\n",
    "    \"\"\"\n",
    "    similarities = similarity_matrix[doc_id]\n",
    "\n",
    "    # Sort by similarity score (descending)\n",
    "    sorted_similarities = sorted(similarities.items(),\n",
    "                                 key=lambda x: x[1],\n",
    "                                 reverse=True)\n",
    "\n",
    "    # Exclude the document itself (similarity = 1.0)\n",
    "    similar_docs = [(doc_id2, score) for doc_id2, score in sorted_similarities\n",
    "                    if doc_id2 != doc_id]\n",
    "\n",
    "    return similar_docs[:top_n]\n"
   ],
   "id": "c44e425def641697",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. RECOMMENDER SYSTEM\n",
    "# =====================\n"
   ],
   "id": "f98cd4d4c833a6da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.249725Z",
     "start_time": "2025-02-26T16:07:14.244976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(query, documents, inverted_index, doc_vectors, tolerance=0.8):\n",
    "    \"\"\"\n",
    "    Search for documents matching a query\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query\n",
    "        documents (dict): Dictionary of documents\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "        doc_vectors (dict): Document vectors for similarity calculations\n",
    "        tolerance (float): Tolerance threshold for fuzzy matching\n",
    "\n",
    "    Returns:\n",
    "        list: Ranked list of matching documents\n",
    "    \"\"\"\n",
    "    # Preprocess the query\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    # If no valid tokens after preprocessing, return empty result\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "\n",
    "    # Find matching documents for each query term\n",
    "    matching_docs = set()\n",
    "\n",
    "    for query_term in query_tokens:\n",
    "        # Try exact matching first\n",
    "        if query_term in inverted_index:\n",
    "            matching_docs.update(inverted_index[query_term])\n",
    "        else:\n",
    "            # Try fuzzy matching if exact match not found\n",
    "            all_terms = list(inverted_index.keys())\n",
    "            close_matches = get_close_matches(query_term, all_terms, n=3, cutoff=tolerance)\n",
    "\n",
    "            for match in close_matches:\n",
    "                matching_docs.update(inverted_index[match])\n",
    "\n",
    "    # If no matching documents found, return empty result\n",
    "    if not matching_docs:\n",
    "        return []\n",
    "\n",
    "    # Calculate query vector\n",
    "    query_vector = {}\n",
    "    for term in query_tokens:\n",
    "        # Use TF-IDF weight if the term is in the corpus, otherwise give it a default weight\n",
    "        query_vector[term] = query_vector.get(term, 0) + 1\n",
    "\n",
    "    # Normalize query vector\n",
    "    query_length = len(query_tokens)\n",
    "    for term in query_vector:\n",
    "        query_vector[term] /= query_length\n",
    "\n",
    "    # Calculate similarity to query for each matching document\n",
    "    similarities = []\n",
    "\n",
    "    for doc_id in matching_docs:\n",
    "        doc_vector = doc_vectors[doc_id]\n",
    "\n",
    "        # Calculate dot product\n",
    "        dot_product = 0\n",
    "        for term, weight in query_vector.items():\n",
    "            if term in doc_vector:\n",
    "                dot_product += weight * doc_vector[term]\n",
    "\n",
    "        # Calculate magnitudes\n",
    "        query_mag = math.sqrt(sum(w**2 for w in query_vector.values()))\n",
    "        doc_mag = math.sqrt(sum(w**2 for w in doc_vector.values()))\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        if query_mag * doc_mag == 0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = dot_product / (query_mag * doc_mag)\n",
    "\n",
    "        similarities.append((doc_id, similarity))\n",
    "\n",
    "    # Sort by similarity score (descending)\n",
    "    ranked_results = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_results\n"
   ],
   "id": "96bb58d8fdc93abb",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.259621Z",
     "start_time": "2025-02-26T16:07:14.255759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_search_results(results, documents, top_n=5):\n",
    "    \"\"\"\n",
    "    Display search results\n",
    "\n",
    "    Args:\n",
    "        results (list): Ranked list of matching documents\n",
    "        documents (dict): Dictionary of documents\n",
    "        top_n (int): Number of results to display\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No matching documents found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(results)} matching documents.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (doc_id, score) in enumerate(results[:top_n]):\n",
    "        doc = documents[doc_id]\n",
    "        title = doc['name']\n",
    "        category = doc['category']\n",
    "\n",
    "        print(f\"Rank {i+1}: {title} [Category: {category}]\")\n",
    "        print(f\"Similarity Score: {score:.4f}\")\n",
    "\n",
    "        # Display snippet (first 150 characters of text)\n",
    "        snippet = doc['text'][:150].strip() + \"...\" if len(doc['text']) > 150 else doc['text']\n",
    "        print(f\"Snippet: {snippet}\")\n",
    "        print(\"-\" * 80)\n"
   ],
   "id": "331cdc77eac17953",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. PERFORMANCE EVALUATION\n",
    "# ========================\n"
   ],
   "id": "305c676608dbc407"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.271730Z",
     "start_time": "2025-02-26T16:07:14.268382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_search(test_queries, documents, inverted_index, doc_vectors):\n",
    "    \"\"\"\n",
    "    Evaluate search performance using test queries\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'avg_precision': []\n",
    "    }\n",
    "\n",
    "    # [Rest of the function remains the same]\n",
    "\n",
    "    # Calculate average metrics - fixed version\n",
    "    avg_metrics = {}\n",
    "    for metric_name in list(metrics.keys()):  # Create a copy of keys for iteration\n",
    "        avg_metrics[f'avg_{metric_name}'] = sum(metrics[metric_name]) / len(metrics[metric_name]) if metrics[metric_name] else 0\n",
    "\n",
    "    # Add average metrics to the original metrics dictionary\n",
    "    metrics.update(avg_metrics)\n",
    "\n",
    "    return metrics"
   ],
   "id": "2f01cc70948c6ac3",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:14.282599Z",
     "start_time": "2025-02-26T16:07:14.278740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_evaluation_results(metrics):\n",
    "    \"\"\"\n",
    "    Display evaluation results\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): Performance metrics\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Search System Performance Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Average Precision: {metrics['avg_precision']:.4f}\")\n",
    "    print(f\"Average Recall: {metrics['avg_recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {metrics['avg_f1_score']:.4f}\")\n",
    "    print(f\"Mean Average Precision (MAP): {metrics['avg_avg_precision']:.4f}\")\n"
   ],
   "id": "be275262283cd375",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Main Execution\n",
    "# =============\n"
   ],
   "id": "aac6822d025f7a6f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-26T16:11:04.281246Z",
     "start_time": "2025-02-26T16:10:58.465713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Set up base directory\n",
    "    base_dir = 'bbc_articles'  # Update this path to your data directory\n",
    "\n",
    "    # 1. Load and preprocess documents\n",
    "    print(\"Loading and preprocessing documents...\")\n",
    "    documents, doc_paths = load_documents(base_dir)\n",
    "    documents = preprocess_documents(documents)\n",
    "\n",
    "    # 2. Build inverted index and calculate TF-IDF\n",
    "    print(\"\\nBuilding inverted index and calculating TF-IDF scores...\")\n",
    "    documents = calculate_term_frequencies(documents)\n",
    "    inverted_index = build_inverted_index(documents)\n",
    "    tfidf, doc_vectors = calculate_tfidf(documents, inverted_index)\n",
    "\n",
    "    # Display inverted index\n",
    "    display_inverted_index(inverted_index)\n",
    "\n",
    "    # 3. Calculate similarity matrix\n",
    "    print(\"\\nCalculating document similarity matrix...\")\n",
    "    similarity_matrix = calculate_similarity_matrix(doc_vectors)\n",
    "\n",
    "    # Display most similar documents for a sample document\n",
    "    sample_doc_id = 0  # Change this to any valid document ID\n",
    "    if documents:\n",
    "        print(f\"\\nMost similar documents to {documents[sample_doc_id]['name']}:\")\n",
    "        similar_docs = get_most_similar_documents(similarity_matrix, sample_doc_id)\n",
    "        for i, (doc_id, score) in enumerate(similar_docs):\n",
    "            print(f\"{i+1}. {documents[doc_id]['name']} (Similarity: {score:.4f})\")\n",
    "\n",
    "    # 4. Test search functionality\n",
    "    print(\"\\nTesting search functionality...\")\n",
    "    test_queries = [\n",
    "        \"technology and artificial intelligence\",\n",
    "        \"business news and economy\",\n",
    "        \"travel destinations in Europe\",\n",
    "        \"art exhibitions and culture\"\n",
    "    ]\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nSearch Query: '{query}'\")\n",
    "        results = search(query, documents, inverted_index, doc_vectors)\n",
    "        display_search_results(results, documents)\n",
    "\n",
    "    # 5. Evaluate search performance\n",
    "    print(\"\\nEvaluating search performance...\")\n",
    "    # Create test queries with relevance judgments\n",
    "    # In a real scenario, you would have a gold standard set of relevance judgments\n",
    "    test_queries_eval = {\n",
    "        0: {\n",
    "            'query': \"technology AI artificial intelligence\",\n",
    "            'relevant_docs': [doc_id for doc_id, doc in documents.items() if doc['category'] == 'technology']\n",
    "        },\n",
    "        1: {\n",
    "            'query': \"business economy finance\",\n",
    "            'relevant_docs': [doc_id for doc_id, doc in documents.items() if doc['category'] == 'business']\n",
    "        },\n",
    "        2: {\n",
    "            'query': \"travel destination Europe\",\n",
    "            'relevant_docs': [doc_id for doc_id, doc in documents.items() if doc['category'] == 'travel']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    metrics = evaluate_search(test_queries_eval, documents, inverted_index, doc_vectors)\n",
    "    display_evaluation_results(metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing documents...\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Extracting text from file\n",
      "Loaded 56 documents from bbc_articles\n",
      "\n",
      "Building inverted index and calculating TF-IDF scores...\n",
      "Top 10 terms in the inverted index:\n",
      "==================================================\n",
      "Term                 Doc Count  Documents           \n",
      "--------------------------------------------------\n",
      "link                 56         [0, 1, 2, 3, 4]...  \n",
      "ago                  56         [0, 1, 2, 3, 4]...  \n",
      "site                 56         [0, 1, 2, 3, 4]...  \n",
      "bbc                  56         [0, 1, 2, 3, 4]...  \n",
      "info                 56         [0, 1, 2, 3, 4]...  \n",
      "help                 56         [0, 1, 2, 3, 4]...  \n",
      "reserv               56         [0, 1, 2, 3, 4]...  \n",
      "Ô¨Åle                  56         [0, 1, 2, 3, 4]...  \n",
      "advertis             56         [0, 1, 2, 3, 4]...  \n",
      "content              56         [0, 1, 2, 3, 4]...  \n",
      "\n",
      "Calculating document similarity matrix...\n",
      "\n",
      "Most similar documents to innovation_innovation_news_10.pdf:\n",
      "1. innovation_innovation_news_8.pdf (Similarity: 0.1443)\n",
      "2. business_business_news_12.pdf (Similarity: 0.1443)\n",
      "3. innovation_innovation_news_5.pdf (Similarity: 0.1309)\n",
      "4. technology_technology_news_5.pdf (Similarity: 0.1309)\n",
      "5. arts_arts_news_4.pdf (Similarity: 0.1235)\n",
      "\n",
      "Testing search functionality...\n",
      "\n",
      "Search Query: 'elon musk'\n",
      "Found 6 matching documents.\n",
      "================================================================================\n",
      "Rank 1: business_business_news_7.pdf [Category: business]\n",
      "Similarity Score: 0.3498\n",
      "Snippet: Who is Doge's official leader? White House says it's not Musk 14 hours ago Kayla Epstein Reuters On Monday afternoon, a federal judge had a simple que...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: business_business_news_0.pdf [Category: business]\n",
      "Similarity Score: 0.1997\n",
      "Snippet: Tesla shares slump after European sales fall 5 hours ago Tom Espiner Business reporter, BBC News Getty Images Shares in electric car maker Tesla have...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: technology_technology_news_2.pdf [Category: technology]\n",
      "Similarity Score: 0.1997\n",
      "Snippet: Tesla shares slump after European sales fall 5 hours ago Tom Espiner Business reporter, BBC News Getty Images Shares in electric car maker Tesla have...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: business_business_news_13.pdf [Category: business]\n",
      "Similarity Score: 0.0350\n",
      "Snippet: Theranos founder Elizabeth Holmes loses fraud appeal 1 day ago Mallory Moench BBC News Getty Images Theranos founder and former chief executive Elizab...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: technology_technology_news_9.pdf [Category: technology]\n",
      "Similarity Score: 0.0246\n",
      "Snippet: Women's abuse online: 'I get trolled every second, every day' 1 day ago Kate Berry & Tom Gerken BBC News Miah Carter Miah Carter has been sharing her...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating search performance...\n",
      "Search System Performance Evaluation\n",
      "==================================================\n",
      "Average Precision: 0.0000\n",
      "Average Recall: 0.0000\n",
      "Average F1 Score: 0.0000\n",
      "Mean Average Precision (MAP): 0.0000\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:07:20.082120Z",
     "start_time": "2025-02-26T16:07:20.080026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "#\n",
    "# def debug_pdf_file():\n",
    "#     \"\"\"Debug reading a specific PDF file using PyPDF2\"\"\"\n",
    "#     file_path = \"bbc_articles/innovation/innovation_news_10.pdf\"\n",
    "#\n",
    "#     # Check if file exists\n",
    "#     if not os.path.exists(file_path):\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "#         return\n",
    "#\n",
    "#     print(f\"File exists: {file_path}\")\n",
    "#     print(f\"File size: {os.path.getsize(file_path)} bytes\")\n",
    "#\n",
    "#     # Try binary mode first to confirm we can read it\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             binary_content = file.read(100)\n",
    "#             print(\"First 100 bytes (binary mode):\", binary_content)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in binary mode: {e}\")\n",
    "#         return\n",
    "#\n",
    "#     # Now try to use PyPDF2 to extract text\n",
    "#     try:\n",
    "#         import PyPDF2\n",
    "#\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             print(\"Opening PDF with PyPDF2...\")\n",
    "#             pdf_reader = PyPDF2.PdfReader(file)\n",
    "#             print(f\"PDF has {len(pdf_reader.pages)} pages\")\n",
    "#\n",
    "#             # Extract text from the first page as a test\n",
    "#             print(\"Extracting text from the first page...\")\n",
    "#             first_page_text = pdf_reader.pages[0].extract_text()\n",
    "#\n",
    "#             # Print first 200 characters of the extracted text\n",
    "#             print(\"First 200 characters of extracted text:\")\n",
    "#             print(first_page_text[:200] if first_page_text else \"No text extracted\")\n",
    "#\n",
    "#             # Try to extract text from all pages\n",
    "#             print(\"Extracting text from all pages...\")\n",
    "#             all_text = \"\"\n",
    "#             for i, page in enumerate(pdf_reader.pages):\n",
    "#                 page_text = page.extract_text()\n",
    "#                 all_text += page_text + \" \"\n",
    "#                 print(f\"Page {i+1}: Extracted {len(page_text)} characters\")\n",
    "#\n",
    "#             print(f\"Total extracted text length: {len(all_text)} characters\")\n",
    "#\n",
    "#     except ImportError:\n",
    "#         print(\"PyPDF2 is not installed. Please install it with: pip install PyPDF2\")\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error using PyPDF2: {e}\")\n",
    "#\n",
    "# # Run the debug function\n",
    "# debug_pdf_file()"
   ],
   "id": "daf05d4cb8c0499c",
   "outputs": [],
   "execution_count": 105
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
