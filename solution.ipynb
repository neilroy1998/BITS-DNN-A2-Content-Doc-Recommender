{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.437758Z",
     "start_time": "2025-02-26T16:20:23.398564Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install PyPDF2 beautifulsoup4 nltk scikit-learn matplotlib",
   "id": "92d2e1437d7de89c",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.473468Z",
     "start_time": "2025-02-26T16:20:23.467226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "4816b6cfa96b7b4c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rajitroy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rajitroy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rajitroy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# ================================"
   ],
   "id": "d2e44442ead75dd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.504541Z",
     "start_time": "2025-02-26T16:20:23.499852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract text content from various file types (HTML, PDF, TXT)\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from file: \", file_path)\n",
    "    try:\n",
    "        # Handle different file types\n",
    "        if file_path.endswith('.html'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Parse HTML with BeautifulSoup\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        elif file_path.endswith('.pdf'):\n",
    "            # Use PyPDF2 for PDF files\n",
    "            import PyPDF2\n",
    "\n",
    "            with open(file_path, 'rb') as file:  # Note the 'rb' mode for binary files\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:  # Some pages might not have extractable text\n",
    "                        text += page_text + \" \"\n",
    "\n",
    "        else:  # For text files\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return \"\""
   ],
   "id": "ca67a4ef42ba45ce",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.526990Z",
     "start_time": "2025-02-26T16:20:23.523954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_documents(base_dir):\n",
    "    \"\"\"\n",
    "    Load all documents from the specified directory structure\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Base directory containing subdirectories for categories\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with document IDs as keys and document info as values\n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    doc_id = 0\n",
    "\n",
    "    # Create dictionary to store document paths\n",
    "    doc_paths = {}\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        category = os.path.basename(root)\n",
    "\n",
    "        for file in files:\n",
    "            # Only process HTML, PDF, and text files\n",
    "            if file.endswith(('.html', '.txt', '.csv', '.pdf')):\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Extract text using our universal extractor for all file types\n",
    "                text = extract_text_from_file(file_path)\n",
    "\n",
    "                # Skip if no text was extracted\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                # Store document info\n",
    "                doc_name = f\"{category}_{file}\"\n",
    "                documents[doc_id] = {\n",
    "                    'id': doc_id,\n",
    "                    'name': doc_name,\n",
    "                    'category': category,\n",
    "                    'path': file_path,\n",
    "                    'text': text,\n",
    "                    'tokens': None,  # Will be populated during preprocessing\n",
    "                    'term_freq': None,  # Will be populated during TF-IDF calculation\n",
    "                }\n",
    "                doc_paths[doc_id] = file_path\n",
    "                doc_id += 1\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents from {base_dir}\")\n",
    "    return documents, doc_paths"
   ],
   "id": "48e8fc91280d38e9",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.546240Z",
     "start_time": "2025-02-26T16:20:23.542319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text: tokenize, remove stopwords, punctuation, and stem\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed tokens\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n"
   ],
   "id": "eeb19eb849502364",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.556829Z",
     "start_time": "2025-02-26T16:20:23.553781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_documents(documents):\n",
    "    \"\"\"\n",
    "    Preprocess all documents in the collection\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary with preprocessed tokens\n",
    "    \"\"\"\n",
    "    for doc_id, doc in documents.items():\n",
    "        doc['tokens'] = preprocess_text(doc['text'])\n",
    "\n",
    "    return documents\n"
   ],
   "id": "ea4df493c448174b",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. INVERTED INDEX AND TF-IDF\n",
    "# ============================\n"
   ],
   "id": "f710acd0c7eb85e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.570576Z",
     "start_time": "2025-02-26T16:20:23.567060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_inverted_index(documents):\n",
    "    \"\"\"\n",
    "    Build an inverted index mapping terms to documents\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Inverted index mapping terms to document IDs\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    for doc_id, doc in documents.items():\n",
    "        # Get unique terms in the document\n",
    "        unique_terms = set(doc['tokens'])\n",
    "\n",
    "        # Add document to the posting list of each term\n",
    "        for term in unique_terms:\n",
    "            inverted_index[term].append(doc_id)\n",
    "\n",
    "    return dict(inverted_index)\n"
   ],
   "id": "b04763661fbd3cc6",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.578272Z",
     "start_time": "2025-02-26T16:20:23.575942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_term_frequencies(documents):\n",
    "    \"\"\"\n",
    "    Calculate term frequencies for each document\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary with term frequencies\n",
    "    \"\"\"\n",
    "    for doc_id, doc in documents.items():\n",
    "        # Count term frequencies\n",
    "        term_freq = Counter(doc['tokens'])\n",
    "        doc['term_freq'] = term_freq\n",
    "\n",
    "    return documents\n"
   ],
   "id": "590e4cc1928727eb",
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.589265Z",
     "start_time": "2025-02-26T16:20:23.585686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_tfidf(documents, inverted_index):\n",
    "    \"\"\"\n",
    "    Calculate TF-IDF scores for all terms in all documents\n",
    "\n",
    "    Args:\n",
    "        documents (dict): Dictionary of documents\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "\n",
    "    Returns:\n",
    "        dict: TF-IDF scores for all terms in all documents\n",
    "        dict: Document vectors for similarity calculations\n",
    "    \"\"\"\n",
    "    N = len(documents)  # Total number of documents\n",
    "\n",
    "    # Calculate IDF for each term\n",
    "    idf = {}\n",
    "    for term, doc_ids in inverted_index.items():\n",
    "        idf[term] = math.log10(N / len(doc_ids))\n",
    "\n",
    "    # Calculate TF-IDF for each term in each document\n",
    "    tfidf = {}\n",
    "    doc_vectors = {}\n",
    "\n",
    "    for doc_id, doc in documents.items():\n",
    "        tfidf[doc_id] = {}\n",
    "        vector = {}\n",
    "\n",
    "        # Get document length (total number of terms)\n",
    "        doc_length = len(doc['tokens'])\n",
    "\n",
    "        # Calculate TF-IDF for each term in the document\n",
    "        for term, freq in doc['term_freq'].items():\n",
    "            # Normalized TF (term frequency / document length)\n",
    "            normalized_tf = freq / doc_length\n",
    "\n",
    "            # TF-IDF score\n",
    "            tfidf[doc_id][term] = normalized_tf * idf.get(term, 0)\n",
    "            vector[term] = tfidf[doc_id][term]\n",
    "\n",
    "        # Store the document vector\n",
    "        doc_vectors[doc_id] = vector\n",
    "\n",
    "    return tfidf, doc_vectors\n"
   ],
   "id": "baafba5dd2874e4b",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.597671Z",
     "start_time": "2025-02-26T16:20:23.594537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_inverted_index(inverted_index, top_n=10):\n",
    "    \"\"\"\n",
    "    Display the inverted index (sorted)\n",
    "\n",
    "    Args:\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "        top_n (int): Number of terms to display\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Sort terms by their frequency (number of documents)\n",
    "    sorted_terms = sorted(inverted_index.items(),\n",
    "                          key=lambda x: len(x[1]),\n",
    "                          reverse=True)\n",
    "\n",
    "    print(f\"Top {top_n} terms in the inverted index:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"{:<20} {:<10} {:<20}\".format(\"Term\", \"Doc Count\", \"Documents\"))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for term, doc_ids in sorted_terms[:top_n]:\n",
    "        print(\"{:<20} {:<10} {:<20}\".format(\n",
    "            term, len(doc_ids), str(doc_ids[:5]) + \"...\" if len(doc_ids) > 5 else str(doc_ids)\n",
    "        ))\n"
   ],
   "id": "74d0527ea3d44aee",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. SIMILARITY CALCULATION\n",
    "# =========================\n"
   ],
   "id": "d07a9058dd5c004"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.606939Z",
     "start_time": "2025-02-26T16:20:23.603312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_similarity_matrix(doc_vectors):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity matrix for document pairs\n",
    "\n",
    "    Args:\n",
    "        doc_vectors (dict): Document vectors\n",
    "\n",
    "    Returns:\n",
    "        dict: Similarity matrix for document pairs\n",
    "    \"\"\"\n",
    "    doc_ids = list(doc_vectors.keys())\n",
    "    similarity_matrix = {}\n",
    "\n",
    "    for i, doc_id1 in enumerate(doc_ids):\n",
    "        similarity_matrix[doc_id1] = {}\n",
    "        vec1 = doc_vectors[doc_id1]\n",
    "\n",
    "        for doc_id2 in doc_ids:\n",
    "            if doc_id1 == doc_id2:\n",
    "                similarity_matrix[doc_id1][doc_id2] = 1.0\n",
    "                continue\n",
    "\n",
    "            vec2 = doc_vectors[doc_id2]\n",
    "\n",
    "            # Calculate dot product\n",
    "            dot_product = 0\n",
    "            for term, tfidf1 in vec1.items():\n",
    "                if term in vec2:\n",
    "                    dot_product += tfidf1 * vec2[term]\n",
    "\n",
    "            # Calculate magnitudes\n",
    "            mag1 = math.sqrt(sum(tfidf**2 for tfidf in vec1.values()))\n",
    "            mag2 = math.sqrt(sum(tfidf**2 for tfidf in vec2.values()))\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            if mag1 * mag2 == 0:\n",
    "                similarity_matrix[doc_id1][doc_id2] = 0\n",
    "            else:\n",
    "                similarity_matrix[doc_id1][doc_id2] = dot_product / (mag1 * mag2)\n",
    "\n",
    "    return similarity_matrix\n"
   ],
   "id": "dc59aa1f6069119f",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.627636Z",
     "start_time": "2025-02-26T16:20:23.623425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_most_similar_documents(similarity_matrix, doc_id, top_n=5):\n",
    "    \"\"\"\n",
    "    Get the most similar documents to a given document\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (dict): Similarity matrix for document pairs\n",
    "        doc_id (int): Document ID\n",
    "        top_n (int): Number of similar documents to return\n",
    "\n",
    "    Returns:\n",
    "        list: Top similar documents with similarity scores\n",
    "    \"\"\"\n",
    "    similarities = similarity_matrix[doc_id]\n",
    "\n",
    "    # Sort by similarity score (descending)\n",
    "    sorted_similarities = sorted(similarities.items(),\n",
    "                                 key=lambda x: x[1],\n",
    "                                 reverse=True)\n",
    "\n",
    "    # Exclude the document itself (similarity = 1.0)\n",
    "    similar_docs = [(doc_id2, score) for doc_id2, score in sorted_similarities\n",
    "                    if doc_id2 != doc_id]\n",
    "\n",
    "    return similar_docs[:top_n]\n"
   ],
   "id": "c44e425def641697",
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. RECOMMENDER SYSTEM\n",
    "# =====================\n"
   ],
   "id": "f98cd4d4c833a6da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.636523Z",
     "start_time": "2025-02-26T16:20:23.631296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(query, documents, inverted_index, doc_vectors, tolerance=0.8):\n",
    "    \"\"\"\n",
    "    Search for documents matching a query\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query\n",
    "        documents (dict): Dictionary of documents\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "        doc_vectors (dict): Document vectors for similarity calculations\n",
    "        tolerance (float): Tolerance threshold for fuzzy matching\n",
    "\n",
    "    Returns:\n",
    "        list: Ranked list of matching documents\n",
    "    \"\"\"\n",
    "    # Preprocess the query\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    # If no valid tokens after preprocessing, return empty result\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "\n",
    "    # Find matching documents for each query term\n",
    "    matching_docs = set()\n",
    "\n",
    "    for query_term in query_tokens:\n",
    "        # Try exact matching first\n",
    "        if query_term in inverted_index:\n",
    "            matching_docs.update(inverted_index[query_term])\n",
    "        else:\n",
    "            # Try fuzzy matching if exact match not found\n",
    "            all_terms = list(inverted_index.keys())\n",
    "            close_matches = get_close_matches(query_term, all_terms, n=3, cutoff=tolerance)\n",
    "\n",
    "            for match in close_matches:\n",
    "                matching_docs.update(inverted_index[match])\n",
    "\n",
    "    # If no matching documents found, return empty result\n",
    "    if not matching_docs:\n",
    "        return []\n",
    "\n",
    "    # Calculate query vector\n",
    "    query_vector = {}\n",
    "    for term in query_tokens:\n",
    "        # Use TF-IDF weight if the term is in the corpus, otherwise give it a default weight\n",
    "        query_vector[term] = query_vector.get(term, 0) + 1\n",
    "\n",
    "    # Normalize query vector\n",
    "    query_length = len(query_tokens)\n",
    "    for term in query_vector:\n",
    "        query_vector[term] /= query_length\n",
    "\n",
    "    # Calculate similarity to query for each matching document\n",
    "    similarities = []\n",
    "\n",
    "    for doc_id in matching_docs:\n",
    "        doc_vector = doc_vectors[doc_id]\n",
    "\n",
    "        # Calculate dot product\n",
    "        dot_product = 0\n",
    "        for term, weight in query_vector.items():\n",
    "            if term in doc_vector:\n",
    "                dot_product += weight * doc_vector[term]\n",
    "\n",
    "        # Calculate magnitudes\n",
    "        query_mag = math.sqrt(sum(w**2 for w in query_vector.values()))\n",
    "        doc_mag = math.sqrt(sum(w**2 for w in doc_vector.values()))\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        if query_mag * doc_mag == 0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = dot_product / (query_mag * doc_mag)\n",
    "\n",
    "        similarities.append((doc_id, similarity))\n",
    "\n",
    "    # Sort by similarity score (descending)\n",
    "    ranked_results = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_results\n"
   ],
   "id": "96bb58d8fdc93abb",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.647378Z",
     "start_time": "2025-02-26T16:20:23.643756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_search_results(results, documents, top_n=5):\n",
    "    \"\"\"\n",
    "    Display search results\n",
    "\n",
    "    Args:\n",
    "        results (list): Ranked list of matching documents\n",
    "        documents (dict): Dictionary of documents\n",
    "        top_n (int): Number of results to display\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No matching documents found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(results)} matching documents.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (doc_id, score) in enumerate(results[:top_n]):\n",
    "        doc = documents[doc_id]\n",
    "        title = doc['name']\n",
    "        category = doc['category']\n",
    "\n",
    "        print(f\"Rank {i+1}: {title} [Category: {category}]\")\n",
    "        print(f\"Similarity Score: {score:.4f}\")\n",
    "\n",
    "        # Display snippet (first 150 characters of text)\n",
    "        snippet = doc['text'][:150].strip() + \"...\" if len(doc['text']) > 150 else doc['text']\n",
    "        print(f\"Snippet: {snippet}\")\n",
    "        print(\"-\" * 80)\n"
   ],
   "id": "331cdc77eac17953",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. PERFORMANCE EVALUATION\n",
    "# ========================\n"
   ],
   "id": "305c676608dbc407"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.658518Z",
     "start_time": "2025-02-26T16:20:23.652311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_search(test_queries, documents, inverted_index, doc_vectors):\n",
    "    \"\"\"\n",
    "    Evaluate search performance using test queries with detailed logging\n",
    "\n",
    "    Args:\n",
    "        test_queries (dict): Dictionary of test queries with relevance judgments\n",
    "        documents (dict): Dictionary of documents\n",
    "        inverted_index (dict): Inverted index mapping terms to document IDs\n",
    "        doc_vectors (dict): Document vectors for similarity calculations\n",
    "\n",
    "    Returns:\n",
    "        dict: Performance metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'query_id': [],\n",
    "        'query': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'avg_precision': []\n",
    "    }\n",
    "\n",
    "    print(\"\\nDETAILED EVALUATION LOGS:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Number of test queries: {len(test_queries)}\")\n",
    "\n",
    "    for query_id, query_info in test_queries.items():\n",
    "        query = query_info['query']\n",
    "        relevant_docs = set(query_info['relevant_docs'])\n",
    "\n",
    "        print(f\"\\nQuery {query_id}: '{query}'\")\n",
    "        print(f\"Number of relevant documents defined: {len(relevant_docs)}\")\n",
    "        if len(relevant_docs) == 0:\n",
    "            print(\"WARNING: No relevant documents defined for this query!\")\n",
    "\n",
    "        # Add query information to metrics\n",
    "        metrics['query_id'].append(query_id)\n",
    "        metrics['query'].append(query)\n",
    "\n",
    "        # Get search results\n",
    "        results = search(query, documents, inverted_index, doc_vectors)\n",
    "        retrieved_docs = set([doc_id for doc_id, _ in results])\n",
    "\n",
    "        print(f\"Number of documents retrieved: {len(retrieved_docs)}\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        if retrieved_docs:\n",
    "            intersection = relevant_docs.intersection(retrieved_docs)\n",
    "            precision = len(intersection) / len(retrieved_docs)\n",
    "            print(f\"Intersection size: {len(intersection)}\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            metrics['precision'].append(precision)\n",
    "        else:\n",
    "            print(\"No documents retrieved!\")\n",
    "            metrics['precision'].append(0)\n",
    "\n",
    "        if relevant_docs:\n",
    "            recall = len(relevant_docs.intersection(retrieved_docs)) / len(relevant_docs)\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            metrics['recall'].append(recall)\n",
    "        else:\n",
    "            print(\"No relevant documents defined!\")\n",
    "            metrics['recall'].append(0)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        if metrics['precision'][-1] + metrics['recall'][-1] > 0:\n",
    "            f1 = 2 * metrics['precision'][-1] * metrics['recall'][-1] / (metrics['precision'][-1] + metrics['recall'][-1])\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "            metrics['f1_score'].append(f1)\n",
    "        else:\n",
    "            print(\"F1 Score: 0.0000 (precision and recall are both 0)\")\n",
    "            metrics['f1_score'].append(0)\n",
    "\n",
    "        # Calculate average precision\n",
    "        avg_precision = 0\n",
    "        correct_count = 0\n",
    "\n",
    "        print(\"\\nPrecision at rank calculation:\")\n",
    "        for i, (doc_id, _) in enumerate(results):\n",
    "            rank = i + 1\n",
    "            if doc_id in relevant_docs:\n",
    "                correct_count += 1\n",
    "                precision_at_k = correct_count / rank\n",
    "                avg_precision += precision_at_k\n",
    "                print(f\"  Rank {rank}: Document {doc_id} is relevant, precision at {rank} = {precision_at_k:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Rank {rank}: Document {doc_id} is not relevant\")\n",
    "\n",
    "        if correct_count > 0 and len(relevant_docs) > 0:\n",
    "            avg_precision /= len(relevant_docs)\n",
    "            print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "        else:\n",
    "            avg_precision = 0\n",
    "            print(\"Average Precision: 0.0000 (no relevant documents retrieved)\")\n",
    "\n",
    "        metrics['avg_precision'].append(avg_precision)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {}\n",
    "    for metric_name in list(metrics.keys()):\n",
    "        if metric_name not in ['query_id', 'query']:  # Skip non-numeric fields\n",
    "            values = metrics[metric_name]\n",
    "            avg_value = sum(values) / len(values) if values else 0\n",
    "            avg_metrics[f'avg_{metric_name}'] = avg_value\n",
    "            print(f\"\\nAverage {metric_name}: {avg_value:.4f}\")\n",
    "\n",
    "    # Add average metrics to the original metrics dictionary\n",
    "    metrics.update(avg_metrics)\n",
    "\n",
    "    return metrics"
   ],
   "id": "2f01cc70948c6ac3",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:23.666261Z",
     "start_time": "2025-02-26T16:20:23.663492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_evaluation_results(metrics):\n",
    "    \"\"\"\n",
    "    Display evaluation results\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): Performance metrics\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Search System Performance Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Average Precision: {metrics['avg_precision']:.4f}\")\n",
    "    print(f\"Average Recall: {metrics['avg_recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {metrics['avg_f1_score']:.4f}\")\n",
    "    print(f\"Mean Average Precision (MAP): {metrics['avg_avg_precision']:.4f}\")\n"
   ],
   "id": "be275262283cd375",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Main Execution\n",
    "# =============\n"
   ],
   "id": "aac6822d025f7a6f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:31.296014Z",
     "start_time": "2025-02-26T16:20:23.673442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Set up base directory\n",
    "    base_dir = 'bbc_articles'  # Update this path to your data directory\n",
    "\n",
    "    # 1. Load and preprocess documents\n",
    "    print(\"Loading and preprocessing documents...\")\n",
    "    documents, doc_paths = load_documents(base_dir)\n",
    "    documents = preprocess_documents(documents)\n",
    "\n",
    "    # 2. Build inverted index and calculate TF-IDF\n",
    "    print(\"\\nBuilding inverted index and calculating TF-IDF scores...\")\n",
    "    documents = calculate_term_frequencies(documents)\n",
    "    inverted_index = build_inverted_index(documents)\n",
    "    tfidf, doc_vectors = calculate_tfidf(documents, inverted_index)\n",
    "\n",
    "    # Display inverted index\n",
    "    display_inverted_index(inverted_index)\n",
    "\n",
    "    # 3. Calculate similarity matrix\n",
    "    print(\"\\nCalculating document similarity matrix...\")\n",
    "    similarity_matrix = calculate_similarity_matrix(doc_vectors)\n",
    "\n",
    "    # Display most similar documents for a sample document\n",
    "    sample_doc_id = 0  # Change this to any valid document ID\n",
    "    if documents:\n",
    "        print(f\"\\nMost similar documents to {documents[sample_doc_id]['name']}:\")\n",
    "        similar_docs = get_most_similar_documents(similarity_matrix, sample_doc_id)\n",
    "        for i, (doc_id, score) in enumerate(similar_docs):\n",
    "            print(f\"{i+1}. {documents[doc_id]['name']} (Similarity: {score:.4f})\")\n",
    "\n",
    "    # 4. Test search functionality\n",
    "    print(\"\\nTesting search functionality...\")\n",
    "    test_queries = [\n",
    "        \"elon musk\",\n",
    "        # \"technology and artificial intelligence\",\n",
    "        # \"business news and economy\",\n",
    "        # \"travel destinations in Europe\",\n",
    "        # \"art exhibitions and culture\"\n",
    "    ]\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nSearch Query: '{query}'\")\n",
    "        results = search(query, documents, inverted_index, doc_vectors)\n",
    "        display_search_results(results, documents)\n",
    "\n",
    "    # 5. Evaluate search performance\n",
    "    print(\"\\nEvaluating search performance...\")\n",
    "    # Create test queries with relevance judgments\n",
    "    # In a real scenario, you would have a gold standard set of relevance judgments\n",
    "\n",
    "    print(\"\\nChecking for Elon Musk and Tesla references in documents...\")\n",
    "    elon_docs = []\n",
    "    for doc_id, doc in documents.items():\n",
    "        text_lower = doc['text'].lower()\n",
    "        if \"elon\" in text_lower or \"musk\" in text_lower or \"tesla\" in text_lower:\n",
    "            elon_docs.append(doc_id)\n",
    "            print(f\"Document {doc_id} ({doc['name']}) contains Elon Musk or Tesla references\")\n",
    "\n",
    "    test_queries_eval = {\n",
    "        0: {\n",
    "            'query': \"technology AI artificial intelligence\",\n",
    "            'relevant_docs': [doc_id for doc_id, doc in documents.items() if doc['category'] == 'technology']\n",
    "        },\n",
    "        1: {\n",
    "            'query': \"business economy finance\",\n",
    "            'relevant_docs': [doc_id for doc_id, doc in documents.items() if doc['category'] == 'business']\n",
    "        },\n",
    "        2: {\n",
    "            'query': \"travel destination Europe\",\n",
    "            'relevant_docs': [doc_id for doc_id, doc in documents.items() if doc['category'] == 'travel']\n",
    "        },\n",
    "        3: {\n",
    "            'query': \"elon musk tesla\",\n",
    "            'relevant_docs': elon_docs\n",
    "        }\n",
    "    }\n",
    "\n",
    "    metrics = evaluate_search(test_queries_eval, documents, inverted_index, doc_vectors)\n",
    "    display_evaluation_results(metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing documents...\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_10.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_8.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_9.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_4.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_5.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_7.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_6.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/innovation/innovation_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_6.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_7.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_5.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_4.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_9.pdf\n",
      "Extracting text from file:  bbc_articles/arts/arts_news_8.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_5.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_4.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_14.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_6.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_7.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_13.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_12.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_10.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_11.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_9.pdf\n",
      "Extracting text from file:  bbc_articles/technology/technology_news_8.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_9.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_8.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_5.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_4.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_6.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_7.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_13.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_12.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_10.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_11.pdf\n",
      "Extracting text from file:  bbc_articles/business/business_news_14.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_0.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_1.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_3.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_2.pdf\n",
      "Extracting text from file:  bbc_articles/travel/travel_news_4.pdf\n",
      "Loaded 56 documents from bbc_articles\n",
      "\n",
      "Building inverted index and calculating TF-IDF scores...\n",
      "Top 10 terms in the inverted index:\n",
      "==================================================\n",
      "Term                 Doc Count  Documents           \n",
      "--------------------------------------------------\n",
      "link                 56         [0, 1, 2, 3, 4]...  \n",
      "ago                  56         [0, 1, 2, 3, 4]...  \n",
      "site                 56         [0, 1, 2, 3, 4]...  \n",
      "bbc                  56         [0, 1, 2, 3, 4]...  \n",
      "info                 56         [0, 1, 2, 3, 4]...  \n",
      "help                 56         [0, 1, 2, 3, 4]...  \n",
      "reserv               56         [0, 1, 2, 3, 4]...  \n",
      "Ô¨Åle                  56         [0, 1, 2, 3, 4]...  \n",
      "advertis             56         [0, 1, 2, 3, 4]...  \n",
      "content              56         [0, 1, 2, 3, 4]...  \n",
      "\n",
      "Calculating document similarity matrix...\n",
      "\n",
      "Most similar documents to innovation_innovation_news_10.pdf:\n",
      "1. innovation_innovation_news_8.pdf (Similarity: 0.1443)\n",
      "2. business_business_news_12.pdf (Similarity: 0.1443)\n",
      "3. innovation_innovation_news_5.pdf (Similarity: 0.1309)\n",
      "4. technology_technology_news_5.pdf (Similarity: 0.1309)\n",
      "5. arts_arts_news_4.pdf (Similarity: 0.1235)\n",
      "\n",
      "Testing search functionality...\n",
      "\n",
      "Search Query: 'elon musk'\n",
      "Found 6 matching documents.\n",
      "================================================================================\n",
      "Rank 1: business_business_news_7.pdf [Category: business]\n",
      "Similarity Score: 0.3498\n",
      "Snippet: Who is Doge's official leader? White House says it's not Musk 14 hours ago Kayla Epstein Reuters On Monday afternoon, a federal judge had a simple que...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2: business_business_news_0.pdf [Category: business]\n",
      "Similarity Score: 0.1997\n",
      "Snippet: Tesla shares slump after European sales fall 5 hours ago Tom Espiner Business reporter, BBC News Getty Images Shares in electric car maker Tesla have...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3: technology_technology_news_2.pdf [Category: technology]\n",
      "Similarity Score: 0.1997\n",
      "Snippet: Tesla shares slump after European sales fall 5 hours ago Tom Espiner Business reporter, BBC News Getty Images Shares in electric car maker Tesla have...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4: business_business_news_13.pdf [Category: business]\n",
      "Similarity Score: 0.0350\n",
      "Snippet: Theranos founder Elizabeth Holmes loses fraud appeal 1 day ago Mallory Moench BBC News Getty Images Theranos founder and former chief executive Elizab...\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5: technology_technology_news_9.pdf [Category: technology]\n",
      "Similarity Score: 0.0246\n",
      "Snippet: Women's abuse online: 'I get trolled every second, every day' 1 day ago Kate Berry & Tom Gerken BBC News Miah Carter Miah Carter has been sharing her...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating search performance...\n",
      "\n",
      "Checking for Elon Musk and Tesla references in documents...\n",
      "Document 5 (innovation_innovation_news_7.pdf) contains Elon Musk or Tesla references\n",
      "Document 10 (innovation_innovation_news_0.pdf) contains Elon Musk or Tesla references\n",
      "Document 24 (technology_technology_news_6.pdf) contains Elon Musk or Tesla references\n",
      "Document 29 (technology_technology_news_2.pdf) contains Elon Musk or Tesla references\n",
      "Document 34 (technology_technology_news_9.pdf) contains Elon Musk or Tesla references\n",
      "Document 36 (business_business_news_9.pdf) contains Elon Musk or Tesla references\n",
      "Document 40 (business_business_news_0.pdf) contains Elon Musk or Tesla references\n",
      "Document 45 (business_business_news_7.pdf) contains Elon Musk or Tesla references\n",
      "Document 46 (business_business_news_13.pdf) contains Elon Musk or Tesla references\n",
      "\n",
      "DETAILED EVALUATION LOGS:\n",
      "======================================================================\n",
      "Number of test queries: 4\n",
      "\n",
      "Query 0: 'technology AI artificial intelligence'\n",
      "Number of relevant documents defined: 15\n",
      "Number of documents retrieved: 31\n",
      "Intersection size: 13\n",
      "Precision: 0.4194\n",
      "Recall: 0.8667\n",
      "F1 Score: 0.5652\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 1 is not relevant\n",
      "  Rank 2: Document 47 is not relevant\n",
      "  Rank 3: Document 0 is not relevant\n",
      "  Rank 4: Document 9 is not relevant\n",
      "  Rank 5: Document 31 is relevant, precision at 5 = 0.2000\n",
      "  Rank 6: Document 39 is not relevant\n",
      "  Rank 7: Document 4 is not relevant\n",
      "  Rank 8: Document 21 is relevant, precision at 8 = 0.2500\n",
      "  Rank 9: Document 25 is relevant, precision at 9 = 0.3333\n",
      "  Rank 10: Document 2 is not relevant\n",
      "  Rank 11: Document 10 is not relevant\n",
      "  Rank 12: Document 24 is relevant, precision at 12 = 0.3333\n",
      "  Rank 13: Document 36 is not relevant\n",
      "  Rank 14: Document 27 is relevant, precision at 14 = 0.3571\n",
      "  Rank 15: Document 37 is not relevant\n",
      "  Rank 16: Document 41 is not relevant\n",
      "  Rank 17: Document 8 is not relevant\n",
      "  Rank 18: Document 26 is relevant, precision at 18 = 0.3333\n",
      "  Rank 19: Document 32 is relevant, precision at 19 = 0.3684\n",
      "  Rank 20: Document 42 is not relevant\n",
      "  Rank 21: Document 46 is not relevant\n",
      "  Rank 22: Document 33 is relevant, precision at 22 = 0.3636\n",
      "  Rank 23: Document 28 is relevant, precision at 23 = 0.3913\n",
      "  Rank 24: Document 23 is relevant, precision at 24 = 0.4167\n",
      "  Rank 25: Document 3 is not relevant\n",
      "  Rank 26: Document 22 is relevant, precision at 26 = 0.4231\n",
      "  Rank 27: Document 18 is not relevant\n",
      "  Rank 28: Document 29 is relevant, precision at 28 = 0.4286\n",
      "  Rank 29: Document 40 is not relevant\n",
      "  Rank 30: Document 5 is not relevant\n",
      "  Rank 31: Document 34 is relevant, precision at 31 = 0.4194\n",
      "Average Precision: 0.3079\n",
      "\n",
      "Query 1: 'business economy finance'\n",
      "Number of relevant documents defined: 15\n",
      "Number of documents retrieved: 29\n",
      "Intersection size: 11\n",
      "Precision: 0.3793\n",
      "Recall: 0.7333\n",
      "F1 Score: 0.5000\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 41 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 48 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 2 is not relevant\n",
      "  Rank 4: Document 37 is relevant, precision at 4 = 0.7500\n",
      "  Rank 5: Document 38 is relevant, precision at 5 = 0.8000\n",
      "  Rank 6: Document 30 is not relevant\n",
      "  Rank 7: Document 1 is not relevant\n",
      "  Rank 8: Document 47 is relevant, precision at 8 = 0.6250\n",
      "  Rank 9: Document 10 is not relevant\n",
      "  Rank 10: Document 24 is not relevant\n",
      "  Rank 11: Document 36 is relevant, precision at 11 = 0.5455\n",
      "  Rank 12: Document 44 is relevant, precision at 12 = 0.5833\n",
      "  Rank 13: Document 0 is not relevant\n",
      "  Rank 14: Document 4 is not relevant\n",
      "  Rank 15: Document 21 is not relevant\n",
      "  Rank 16: Document 27 is not relevant\n",
      "  Rank 17: Document 29 is not relevant\n",
      "  Rank 18: Document 40 is relevant, precision at 18 = 0.4444\n",
      "  Rank 19: Document 8 is not relevant\n",
      "  Rank 20: Document 26 is not relevant\n",
      "  Rank 21: Document 19 is not relevant\n",
      "  Rank 22: Document 49 is relevant, precision at 22 = 0.4091\n",
      "  Rank 23: Document 55 is not relevant\n",
      "  Rank 24: Document 46 is relevant, precision at 24 = 0.4167\n",
      "  Rank 25: Document 25 is not relevant\n",
      "  Rank 26: Document 50 is relevant, precision at 26 = 0.4231\n",
      "  Rank 27: Document 52 is not relevant\n",
      "  Rank 28: Document 3 is not relevant\n",
      "  Rank 29: Document 22 is not relevant\n",
      "Average Precision: 0.4665\n",
      "\n",
      "Query 2: 'travel destination Europe'\n",
      "Number of relevant documents defined: 5\n",
      "Number of documents retrieved: 27\n",
      "Intersection size: 5\n",
      "Precision: 0.1852\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.3125\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 53 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 55 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 48 is not relevant\n",
      "  Rank 4: Document 51 is relevant, precision at 4 = 0.7500\n",
      "  Rank 5: Document 14 is not relevant\n",
      "  Rank 6: Document 52 is relevant, precision at 6 = 0.6667\n",
      "  Rank 7: Document 2 is not relevant\n",
      "  Rank 8: Document 37 is not relevant\n",
      "  Rank 9: Document 13 is not relevant\n",
      "  Rank 10: Document 29 is not relevant\n",
      "  Rank 11: Document 40 is not relevant\n",
      "  Rank 12: Document 43 is not relevant\n",
      "  Rank 13: Document 49 is not relevant\n",
      "  Rank 14: Document 46 is not relevant\n",
      "  Rank 15: Document 54 is relevant, precision at 15 = 0.3333\n",
      "  Rank 16: Document 44 is not relevant\n",
      "  Rank 17: Document 3 is not relevant\n",
      "  Rank 18: Document 22 is not relevant\n",
      "  Rank 19: Document 45 is not relevant\n",
      "  Rank 20: Document 38 is not relevant\n",
      "  Rank 21: Document 25 is not relevant\n",
      "  Rank 22: Document 5 is not relevant\n",
      "  Rank 23: Document 8 is not relevant\n",
      "  Rank 24: Document 16 is not relevant\n",
      "  Rank 25: Document 18 is not relevant\n",
      "  Rank 26: Document 26 is not relevant\n",
      "  Rank 27: Document 34 is not relevant\n",
      "Average Precision: 0.7500\n",
      "\n",
      "Query 3: 'elon musk tesla'\n",
      "Number of relevant documents defined: 9\n",
      "Number of documents retrieved: 6\n",
      "Intersection size: 6\n",
      "Precision: 1.0000\n",
      "Recall: 0.6667\n",
      "F1 Score: 0.8000\n",
      "\n",
      "Precision at rank calculation:\n",
      "  Rank 1: Document 40 is relevant, precision at 1 = 1.0000\n",
      "  Rank 2: Document 29 is relevant, precision at 2 = 1.0000\n",
      "  Rank 3: Document 45 is relevant, precision at 3 = 1.0000\n",
      "  Rank 4: Document 46 is relevant, precision at 4 = 1.0000\n",
      "  Rank 5: Document 34 is relevant, precision at 5 = 1.0000\n",
      "  Rank 6: Document 5 is relevant, precision at 6 = 1.0000\n",
      "Average Precision: 0.6667\n",
      "\n",
      "Average precision: 0.4960\n",
      "\n",
      "Average recall: 0.8167\n",
      "\n",
      "Average f1_score: 0.5444\n",
      "\n",
      "Average avg_precision: 0.5478\n",
      "Search System Performance Evaluation\n",
      "==================================================\n",
      "Average Precision: 0.4960\n",
      "Average Recall: 0.8167\n",
      "Average F1 Score: 0.5444\n",
      "Mean Average Precision (MAP): 0.5478\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T16:20:31.315354Z",
     "start_time": "2025-02-26T16:20:31.311752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "#\n",
    "# def debug_pdf_file():\n",
    "#     \"\"\"Debug reading a specific PDF file using PyPDF2\"\"\"\n",
    "#     file_path = \"bbc_articles/innovation/innovation_news_10.pdf\"\n",
    "#\n",
    "#     # Check if file exists\n",
    "#     if not os.path.exists(file_path):\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "#         return\n",
    "#\n",
    "#     print(f\"File exists: {file_path}\")\n",
    "#     print(f\"File size: {os.path.getsize(file_path)} bytes\")\n",
    "#\n",
    "#     # Try binary mode first to confirm we can read it\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             binary_content = file.read(100)\n",
    "#             print(\"First 100 bytes (binary mode):\", binary_content)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in binary mode: {e}\")\n",
    "#         return\n",
    "#\n",
    "#     # Now try to use PyPDF2 to extract text\n",
    "#     try:\n",
    "#         import PyPDF2\n",
    "#\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             print(\"Opening PDF with PyPDF2...\")\n",
    "#             pdf_reader = PyPDF2.PdfReader(file)\n",
    "#             print(f\"PDF has {len(pdf_reader.pages)} pages\")\n",
    "#\n",
    "#             # Extract text from the first page as a test\n",
    "#             print(\"Extracting text from the first page...\")\n",
    "#             first_page_text = pdf_reader.pages[0].extract_text()\n",
    "#\n",
    "#             # Print first 200 characters of the extracted text\n",
    "#             print(\"First 200 characters of extracted text:\")\n",
    "#             print(first_page_text[:200] if first_page_text else \"No text extracted\")\n",
    "#\n",
    "#             # Try to extract text from all pages\n",
    "#             print(\"Extracting text from all pages...\")\n",
    "#             all_text = \"\"\n",
    "#             for i, page in enumerate(pdf_reader.pages):\n",
    "#                 page_text = page.extract_text()\n",
    "#                 all_text += page_text + \" \"\n",
    "#                 print(f\"Page {i+1}: Extracted {len(page_text)} characters\")\n",
    "#\n",
    "#             print(f\"Total extracted text length: {len(all_text)} characters\")\n",
    "#\n",
    "#     except ImportError:\n",
    "#         print(\"PyPDF2 is not installed. Please install it with: pip install PyPDF2\")\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error using PyPDF2: {e}\")\n",
    "#\n",
    "# # Run the debug function\n",
    "# debug_pdf_file()"
   ],
   "id": "daf05d4cb8c0499c",
   "outputs": [],
   "execution_count": 161
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
